{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d3ceb07fcdfc44",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pipeline Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505bde399cd0be2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130fe1a34bbf0c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:36:52.315263900Z",
     "start_time": "2024-02-01T17:36:49.746531600Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import handwriting.preprocess as preprocess\n",
    "import cv2\n",
    "import utils.configs as Config\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "310009fa9893b0cc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config_path = \"utils/configs.json\"\n",
    "config = Config.Config(config_path)\n",
    "\n",
    "# Pipeline Parameter\n",
    "PLOTTING = config.get_pipeline_parameter()[\"plotting\"]\n",
    "if PLOTTING == \"True\":\n",
    "    PLOTTING = True\n",
    "else:\n",
    "    PLOTTING = False\n",
    "CUT_TOP = config.get_pipeline_parameter()[\"bb_model_cut_top\"]\n",
    "if CUT_TOP == \"True\":\n",
    "    CUT_TOP = True\n",
    "else:\n",
    "    CUT_TOP = False\n",
    "    \n",
    "BB_MODEL_NO = config.get_pipeline_parameter()[\"bb_model\"]\n",
    "HANDWRITING_MODEL_NO = config.get_pipeline_parameter()[\"handwriting_model\"] # 1 = IAM Trained no transfer learning\n",
    "NUMBER_MODEL_NO = config.get_pipeline_parameter()[\"number_model\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T17:36:52.323234100Z",
     "start_time": "2024-02-01T17:36:52.317265500Z"
    }
   },
   "id": "6f6903af6587b81c",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "97f29d7aacc4aa39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bounding Box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc611d575994aa8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bounding Box Modell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b33510e14260b8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Show Image Real Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3b74effcc816bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:36:55.845610700Z",
     "start_time": "2024-02-01T17:36:52.321227200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "from bounding_box.model import load_weight_model,predict_image,get_image_as_array, show_image \n",
    "from bounding_box.config import NUM_CLASSES_ALL,BBOX_PATH,MAIN_BBOX_DETECTOR_MODEL,SUB_BBOX_DETECTOR_MODEL  \n",
    "from bounding_box.model import load_weight_model, predict_image,plot_image, get_templated_data, edit_sub_boxes_cut_links, edit_sub_boxes_cut_top\n",
    "from bounding_box.template import build_templating_data\n",
    "from bounding_box.ressize import scale_up\n",
    "\n",
    "bbox_model = load_weight_model(r\"bounding_box\\workspace\\models\\main_bbox_detector_model.h5\",4)\n",
    "image_path = \"data_zettel/optimal_page/nathan_optimal.jpg\"\n",
    "original_image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc395187e28060a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2789ea3b30ead91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:02.103970500Z",
     "start_time": "2024-02-01T17:36:55.844608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n"
     ]
    }
   ],
   "source": [
    "main_boxes, confidence, classes, ratios = predict_image(image_path, bbox_model)\n",
    "\n",
    "if PLOTTING:\n",
    "    show_image(image_path, main_boxes, confidence, classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48741c46959868",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8541cad23f3e1cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:02.127423600Z",
     "start_time": "2024-02-01T17:37:02.105971200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "537b0687f2ff44b782d7681ff8a30547"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0349c29f7dff4d779f10821117c303dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "org_ms_boxes_person, org_ms_boxes_wohnsitz, org_ms_boxes_ausbildung, org_ms_boxes_wwa, person_class_ids, ausbildung_class_ids, wohnsitz_class_ids, wwa_class_ids, widthOrgImag, heightOrgImag = build_templating_data()\n",
    "\n",
    "ausbildung, person, wohnsitz, wwa, best_predicted = get_templated_data(main_boxes, confidence, classes, org_ms_boxes_person,\n",
    "                                                                       org_ms_boxes_wohnsitz, org_ms_boxes_ausbildung,\n",
    "                                                                       org_ms_boxes_wwa, person_class_ids,\n",
    "                                                                       ausbildung_class_ids, wohnsitz_class_ids,\n",
    "                                                                       wwa_class_ids)\n",
    "\n",
    "if CUT_TOP:\n",
    "    ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut = edit_sub_boxes_cut_top(ausbildung, person, wohnsitz, wwa)\n",
    "else:\n",
    "    ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut = edit_sub_boxes_cut_links(ausbildung, person, wohnsitz, wwa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543883486dd8288a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scale Templating up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8bc111012269cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:02.133936100Z",
     "start_time": "2024-02-01T17:37:02.131937700Z"
    }
   },
   "outputs": [],
   "source": [
    "ausbildung_cut_scaled, person_cut_scaled, wohnsitz_cut_scaled, wwa_cut_scaled = scale_up( ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut, ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f60bd61464db3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:02.152411100Z",
     "start_time": "2024-02-01T17:37:02.133936100Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_boxes = ausbildung_cut_scaled[0] + person_cut_scaled[0] +  wohnsitz_cut_scaled[0] +  wwa_cut_scaled[0] \n",
    "sub_classes = ausbildung_cut_scaled[1] + person_cut_scaled[1] + wohnsitz_cut_scaled[1] +  wwa_cut_scaled[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb406f49e1a476e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:02.152411100Z",
     "start_time": "2024-02-01T17:37:02.139758200Z"
    }
   },
   "outputs": [],
   "source": [
    "if PLOTTING:\n",
    "    plot_image(image_path, ausbildung_cut_scaled, person_cut_scaled, wohnsitz_cut_scaled, wwa_cut_scaled, best_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580ef38cc7c8ffc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ROI Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bcfb0dfd70010cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.067566900Z",
     "start_time": "2024-02-01T17:37:02.147898100Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "ImageInfo = namedtuple('ImageInfo', ['image', 'sub_class','value'])\n",
    "\n",
    "# Crop ROI\n",
    "import cv2\n",
    "from bounding_box.ressize import resize_imaged_without_expand_dim\n",
    "from bounding_box.config import YOLO_WIDTH, YOLO_HEIGHT\n",
    "def crop(xmin, ymin, xmax, ymax, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    #image = resize_imaged_without_expand_dim(image, YOLO_WIDTH, YOLO_HEIGHT)\n",
    "    xmin = int(round(xmin))\n",
    "    ymin = int(round(ymin))\n",
    "    xmax = int(round(xmax))\n",
    "    ymax = int(round(ymax))\n",
    "    # width = int(round(width))\n",
    "    # height = int(round(height))\n",
    "    # rowBeg = y\n",
    "    # rowEnd = y + height\n",
    "    # columnBeg = x\n",
    "    # columnEnd = x + width\n",
    "    imgCropped = image[ymin:ymax, xmin:xmax]\n",
    "    return imgCropped\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "images_info_cropped = []\n",
    "for i,box in enumerate(sub_boxes):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    imgCropped = crop(xmin, ymin, xmax, ymax, image_path)\n",
    "    image_info = ImageInfo(image=imgCropped,sub_class=sub_classes[i],value=\"\")\n",
    "    images_info_cropped.append(image_info)\n",
    "    if PLOTTING:\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(imgCropped)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2274dd56bfb7ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32d50dd270d79",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a886d9d68004c435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.316635200Z",
     "start_time": "2024-02-01T17:37:04.069073900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "IMAGE_WIDTH = config.get_model_parameter()[\"width\"] # default: 1024\n",
    "IMAGE_HEIGHT = config.get_model_parameter()[\"height\"] # default: 128\n",
    "\n",
    "img_size=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "preprocessed_image_infos = []\n",
    "for image_info in images_info_cropped:\n",
    "    image = image_info.image \n",
    "    image = np.mean(image, axis=2, keepdims=True)\n",
    "    image = preprocess.distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    temp_sub_class = image_info.sub_class\n",
    "    temp_image_info = ImageInfo(image=image,sub_class=temp_sub_class,value=\"\")\n",
    "    preprocessed_image_infos.append(temp_image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587cee9abf935b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfbcac4daa071012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.321141700Z",
     "start_time": "2024-02-01T17:37:04.317631400Z"
    }
   },
   "outputs": [],
   "source": [
    "if PLOTTING:   \n",
    "    plot_image = preprocessed_image_infos[9].image\n",
    "    plot_image = np.transpose(plot_image, (1, 0, 2))\n",
    "    plot_image = np.flipud(plot_image)\n",
    "    plt.imshow(plot_image[:, :, 0],cmap='gray')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098d538d74bffb4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Handwriting Recognition Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bf781cee4e3fd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.330857400Z",
     "start_time": "2024-02-01T17:37:04.321141700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded max_len: 93\n",
      "Loaded characters: ['!', '\"', '#', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|']\n",
      "Loaded number max_len: 15\n",
      "Loaded number characters: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "\"\"\" import handwriting.tokenizer as tokenizer \"\"\"\n",
    "import pickle\n",
    "# Load from pickle file\n",
    "IAM = HANDWRITING_MODEL_NO == 1\n",
    "if IAM:\n",
    "    with open('iam_handwriting_model_characters.pkl', 'rb') as file:\n",
    "        loaded_max_len, loaded_characters = pickle.load(file)\n",
    "else:\n",
    "    with open('bafog_handwriting_model_characters.pkl', 'rb') as file:\n",
    "        loaded_max_len, loaded_characters = pickle.load(file)\n",
    "    \n",
    "with open('number_saved_max_len_char.pkl', 'rb') as file:\n",
    "     max_len_num, character_num = pickle.load(file)\n",
    "# Print loaded data\n",
    "print(\"Loaded max_len:\", loaded_max_len)\n",
    "print(\"Loaded characters:\", loaded_characters)\n",
    "\n",
    "print(\"Loaded number max_len:\", max_len_num)\n",
    "print(\"Loaded number characters:\", character_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a056aa12c2bc4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:04.365345600Z",
     "start_time": "2024-02-01T17:37:04.332856900Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import StringLookup\n",
    "char_to_num = StringLookup(vocabulary=list(loaded_characters), mask_token=None)\n",
    "num_to_char = StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "number_char_to_num = StringLookup(vocabulary=list(character_num), mask_token=None)\n",
    "number_num_to_char = StringLookup(vocabulary=number_char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :loaded_max_len]\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "def decode_number_pred(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_len_num]\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(number_num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "def load_model_numbers():\n",
    "    model_weight_path = \"models/only_numbers/transferlearningTestingModel_weights.keras\"\n",
    "    model_path = \"models/only_numbers/\"\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model and weights...\")\n",
    "        model = load_model(model_path)\n",
    "        model.load_weights(model_weight_path)\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No pre-trained model or weights found.\")\n",
    "        return None\n",
    "def load_model_and_weights():\n",
    "    if IAM:\n",
    "        model_weight_path = \"models/model9v3_xl/model9v3_xl_weights.keras\"\n",
    "        model_path = \"models/model9v3_xl/\"\n",
    "    else:\n",
    "        model_weight_path = \"models/dense_and_full/transferlearningTestingModel_weights.keras\"\n",
    "        model_path = 'models/dense_and_full/'\n",
    "        # model_weight_path = \"models/denselayer1/transferlearningTestingModel_weights.keras\"\n",
    "        # model_path = 'models/denselayer1/'\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model and weights...\")\n",
    "        model = load_model(model_path)\n",
    "        model.load_weights(model_weight_path)\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No pre-trained model or weights found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd0422d0138b0426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:58.875702600Z",
     "start_time": "2024-02-01T17:37:04.359234100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/model9v3_xl/\n",
      "Loading pre-trained model and weights...\n",
      "Model and weights loaded successfully.\n",
      "models/only_numbers/\n",
      "Loading pre-trained model and weights...\n",
      "Model and weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Handwriting\n",
    "handwriting_model = load_model_and_weights()\n",
    "prediction_model = keras.models.Model(handwriting_model.get_layer(name=\"image\").input, handwriting_model.get_layer(name=\"dense2\").output)\n",
    "\n",
    "number_model = load_model_numbers()\n",
    "number_pred_model = keras.models.Model(number_model.get_layer(name=\"image\").input, number_model.get_layer(name=\"dense2\").output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4255bf9e79b95c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dd6c439beb77c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:58.876702Z",
     "start_time": "2024-02-01T17:37:58.863849500Z"
    }
   },
   "outputs": [],
   "source": [
    "def spell_checker(text):\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker(language='de')\n",
    "    words = [word for word in text.split(\" \") if word != '']\n",
    "    #Spellchecker\n",
    "    corrected_text = ' '.join([spell.correction(word) if spell.correction(word) is not None else word for word in text.split()])\n",
    "    return corrected_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106363bb404ff1cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Map Sub_Classes to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1ec31a7c3c4b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:37:58.962041Z",
     "start_time": "2024-02-01T17:37:58.867191400Z"
    }
   },
   "outputs": [],
   "source": [
    "import bounding_box.config as bounding_box_config\n",
    "class_ids = bounding_box_config.class_ids\n",
    "\n",
    "def map_sub_class_to_string_and_sort(class_number):\n",
    "    temp_class_string = class_ids[class_number]\n",
    "    \n",
    "    not_class_list = [\"Ausbildung_Klasse\",\"Ausbildung\",\"Person\",\"Wohnsitz\",\"Wohnsitz_waehrend_Ausbildung\"]\n",
    "    if temp_class_string not in not_class_list:\n",
    "        return temp_class_string\n",
    "    return -1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95cad03f941a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4145288b4b15eaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:38:06.530216800Z",
     "start_time": "2024-02-01T17:37:58.961044200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja X Count: 50193 Nein X Count: 54045\n",
      "Ja X Count: 43284 Nein X Count: 49046\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Ja X Count: 27203 Nein X Count: 28873\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "images_with_value = []\n",
    "# Prediction\n",
    "for i, preprocess_image in enumerate(preprocessed_image_infos):\n",
    "    temp_sub_class = preprocess_image.sub_class\n",
    "    temp_sub_class_string = map_sub_class_to_string_and_sort(temp_sub_class)\n",
    "    number_sub_classes = ['Wohnsitz_waehrend_Ausbildung_Hausnummer','Wohnsitz_waehrend_Ausbildung_Postleitzahl','Wohnsitz_Hausnummer',\n",
    "                          'Wohnsitz_Postleitzahl','Person_Geburtsdatum','Person_Familienstand_seit','Ausbildung_Foerderungsnummer']\n",
    "    check_boxes_sub_classes =['Ausbildung_Antrag_gestellt_ja','Ausbildung_Vollzeit','Person_Kinder'] # hier fehlen noch welche sind aber noch nicht im templating\n",
    "    \n",
    "    if temp_sub_class_string != -1:\n",
    "        if temp_sub_class_string in number_sub_classes:\n",
    "            preds = number_pred_model.predict(tf.expand_dims(preprocess_image.image, axis=0))\n",
    "            pred_texts = decode_number_pred(preds)\n",
    "            number_prediction = pred_texts[0]\n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=number_prediction)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        elif temp_sub_class_string in check_boxes_sub_classes:\n",
    "            import contrast_true_or_false.contrast_tof as check_box_checker\n",
    "            from PIL import Image\n",
    "            numpy_array = preprocess_image.image.numpy()\n",
    "            numpy_array = numpy_array.squeeze()\n",
    "            numpy_array = np.clip(numpy_array, 0.0, 1.0)\n",
    "            image_array_uint8 = (numpy_array * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(image_array_uint8)\n",
    "            result = check_box_checker.is_checkbox_checked(pil_image, PLOTTING)\n",
    "            \n",
    "            \n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=result)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        else:\n",
    "            preds = prediction_model.predict(tf.expand_dims(preprocess_image.image, axis=0))\n",
    "            pred_texts = decode_batch_predictions(preds)\n",
    "            selected_pred_text = pred_texts[0]\n",
    "            selected_pred_text = selected_pred_text.replace(\"|\",\" \")\n",
    "            prediction_text = spell_checker(selected_pred_text)\n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=prediction_text)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd322389744760",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot Predicted Text and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417442424e90aef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:38:06.537323500Z",
     "start_time": "2024-02-01T17:38:06.533216400Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_evaluation(images, rows=5, cols=None):\n",
    "    num_images = len(images)\n",
    "    if cols is None:\n",
    "        cols = -(-num_images // rows)  # Ceiling division to calculate the number of columns\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(25, 5))  # Adjust the figsize as needed\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        pred_texts = image.value\n",
    "        title = f\"{image.sub_class}: {pred_texts}\"\n",
    "        plot_image = image.image\n",
    "        \n",
    "        plot_image = np.transpose(plot_image, (1, 0, 2))\n",
    "        plot_image = np.flipud(plot_image)\n",
    "\n",
    "        # Calculate the position of the subplot in the grid\n",
    "        row_pos, col_pos = divmod(i, cols)\n",
    "\n",
    "        # Use the appropriate axis for each subplot\n",
    "        ax = axes[row_pos, col_pos] if num_images > 1 else axes\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(plot_image[:, :, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot parameters for better layout\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aa8216da41b32f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-01T17:38:28.759162300Z",
     "start_time": "2024-02-01T17:38:28.698523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausbildung_Antrag_gestellt_ja - Ja\n",
      "Ausbildung_Vollzeit - Ja\n",
      "Ausbilung_Abschluss - Master\n",
      "Ausbildung_Foerderungsnummer - 24003343077020\n",
      "Ausbildung_Staette - ochachure vorn\n",
      "Ausbildung_Amt - \n",
      "Person_Familienstand - bis .\n",
      "Person_Kinder - Ja\n",
      "Person_Stattsangehörigkeit_Ehegatte - Poooooorsrsssreistersooons\n",
      "Person_Stattsangehörigkeit_eigene - De\n",
      "Person_Familienstand_seit - 22272998\n",
      "Person_Geburtsdatum - 212002\n",
      "Person_Geburtsort - a wie\n",
      "Person_Geburtsname - aßen .\n",
      "Person_Vorname - a kiners.\n",
      "Person_Name - ihn .\n",
      "Wohnsitz_Adresszusatz - so .\n",
      "Wohnsitz_Land - this !\n",
      "Wohnsitz_Ort - ohne\n",
      "Wohnsitz_Postleitzahl - 3\n",
      "Wohnsitz_Hausnummer - 22\n",
      "Wohnsitz_Strasse - ohne ser\n",
      "Wohnsitz_waehrend_Ausbildung_Adresszusatz - days\n",
      "Wohnsitz_waehrend_Ausbildung_Land - seit\n",
      "Wohnsitz_waehrend_Ausbildung_Ort - \" binnen\n",
      "Wohnsitz_waehrend_Ausbildung_Postleitzahl - 42389\n",
      "Wohnsitz_waehrend_Ausbildung_Hausnummer - 22\n",
      "Wohnsitz_waehrend_Ausbildung_Strasse - einen\n"
     ]
    }
   ],
   "source": [
    "if PLOTTING:\n",
    "    plot_evaluation(images_with_value)\n",
    "else:\n",
    "    for image in images_with_value:\n",
    "        print(f\"{image.sub_class} - {image.value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc221f28e0156f75",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pipeline Notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0283c8fd51e13a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:47.987902300Z",
     "start_time": "2024-02-03T12:28:47.936723200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505bde399cd0be2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "130fe1a34bbf0c03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:47.997034500Z",
     "start_time": "2024-02-03T12:28:47.991523700Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import handwriting.preprocess as preprocess\n",
    "import cv2\n",
    "import utils.configs as Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310009fa9893b0cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6903af6587b81c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:48.003235700Z",
     "start_time": "2024-02-03T12:28:47.999047600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_path = \"utils/configs.json\"\n",
    "config = Config.Config(config_path)\n",
    "\n",
    "# Pipeline Parameter\n",
    "PLOTTING = config.get_pipeline_parameter()[\"plotting\"]\n",
    "if PLOTTING == \"True\":\n",
    "    PLOTTING = True\n",
    "else:\n",
    "    PLOTTING = False\n",
    "    \n",
    "    \n",
    "CUT_TOP = config.get_pipeline_parameter()[\"bb_model_cut_top\"]\n",
    "if CUT_TOP == \"True\":\n",
    "    CUT_TOP = True\n",
    "else:\n",
    "    CUT_TOP = False\n",
    "\n",
    "\n",
    "# Image Width und Height\n",
    "IMAGE_WIDTH = config.get_model_parameter()[\"width\"] # default: 1024\n",
    "IMAGE_HEIGHT = config.get_model_parameter()[\"height\"] # default: 128\n",
    "\n",
    "# Model Auswahl\n",
    "BB_MODEL_NO = config.get_pipeline_parameter()[\"bb_model\"]\n",
    "HANDWRITING_MODEL_NO = config.get_pipeline_parameter()[\"handwriting_model\"] # 1 = IAM Trained no transfer learning\n",
    "NUMBER_MODEL_NO = config.get_pipeline_parameter()[\"number_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f29d7aacc4aa39",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bounding Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e3b74effcc816bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:48.015807700Z",
     "start_time": "2024-02-03T12:28:48.004236500Z"
    }
   },
   "outputs": [],
   "source": [
    "from bounding_box.model import load_weight_model,predict_image,get_image_as_array, show_image \n",
    "from bounding_box.config import NUM_CLASSES_ALL,BBOX_PATH,MAIN_BBOX_DETECTOR_MODEL,SUB_BBOX_DETECTOR_MODEL  \n",
    "from bounding_box.model import load_weight_model, predict_image,plot_image, get_templated_data, edit_sub_boxes_cut_links, edit_sub_boxes_cut_top\n",
    "from bounding_box.template import build_templating_data\n",
    "from bounding_box.ressize import scale_up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc247279cc36d3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "BB Model & Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c35c28f96c0e790c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:51.686849Z",
     "start_time": "2024-02-03T12:28:48.011807400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bbox_model = load_weight_model(r\"bounding_box\\workspace\\models\\main_bbox_detector_model.h5\",4)\n",
    "image_path = \"data_zettel/optimal_page/nathan_optimal.jpg\"\n",
    "original_image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc395187e28060a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2789ea3b30ead91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:55.499542Z",
     "start_time": "2024-02-03T12:28:51.688856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 20s 20s/step\n"
     ]
    }
   ],
   "source": [
    "main_boxes, confidence, classes, ratios = predict_image(image_path, bbox_model)\n",
    "\n",
    "if PLOTTING:\n",
    "    show_image(image_path, main_boxes, confidence, classes)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48741c46959868",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8541cad23f3e1cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:55.534206200Z",
     "start_time": "2024-02-03T12:28:55.499542Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.47it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.76it/s]\n"
     ]
    }
   ],
   "source": [
    "org_ms_boxes_person, org_ms_boxes_wohnsitz, org_ms_boxes_ausbildung, org_ms_boxes_wwa, person_class_ids, ausbildung_class_ids, wohnsitz_class_ids, wwa_class_ids, widthOrgImag, heightOrgImag = build_templating_data()\n",
    "\n",
    "ausbildung, person, wohnsitz, wwa, best_predicted = get_templated_data(main_boxes, confidence, classes, org_ms_boxes_person,\n",
    "                                                                       org_ms_boxes_wohnsitz, org_ms_boxes_ausbildung,\n",
    "                                                                       org_ms_boxes_wwa, person_class_ids,\n",
    "                                                                       ausbildung_class_ids, wohnsitz_class_ids,\n",
    "                                                                       wwa_class_ids)\n",
    "\n",
    "if CUT_TOP:\n",
    "    ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut = edit_sub_boxes_cut_top(ausbildung, person, wohnsitz, wwa)\n",
    "else:\n",
    "    ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut = edit_sub_boxes_cut_links(ausbildung, person, wohnsitz, wwa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543883486dd8288a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scale Templating up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8bc111012269cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:55.535198200Z",
     "start_time": "2024-02-03T12:28:55.520163Z"
    }
   },
   "outputs": [],
   "source": [
    "ausbildung_cut_scaled, person_cut_scaled, wohnsitz_cut_scaled, wwa_cut_scaled = scale_up( ausbildung_cut, person_cut, wohnsitz_cut, wwa_cut, ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20f60bd61464db3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:55.536711400Z",
     "start_time": "2024-02-03T12:28:55.522160400Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_boxes = ausbildung_cut_scaled[0] + person_cut_scaled[0] +  wohnsitz_cut_scaled[0] +  wwa_cut_scaled[0] \n",
    "sub_classes = ausbildung_cut_scaled[1] + person_cut_scaled[1] + wohnsitz_cut_scaled[1] +  wwa_cut_scaled[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb406f49e1a476e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:55.548748Z",
     "start_time": "2024-02-03T12:28:55.529206400Z"
    }
   },
   "outputs": [],
   "source": [
    "if PLOTTING:\n",
    "    plot_image(image_path, ausbildung_cut_scaled, person_cut_scaled, wohnsitz_cut_scaled, wwa_cut_scaled, best_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7580ef38cc7c8ffc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ROI Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bcfb0dfd70010cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:57.491432100Z",
     "start_time": "2024-02-03T12:28:55.538223600Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "ImageInfo = namedtuple('ImageInfo', ['image', 'sub_class','value'])\n",
    "\n",
    "# Crop ROI\n",
    "import cv2\n",
    "def crop(xmin, ymin, xmax, ymax, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    xmin = int(round(xmin))\n",
    "    ymin = int(round(ymin))\n",
    "    xmax = int(round(xmax))\n",
    "    ymax = int(round(ymax))\n",
    "    imgCropped = image[ymin:ymax, xmin:xmax]\n",
    "    return imgCropped\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "images_info_cropped = []\n",
    "for i,box in enumerate(sub_boxes):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    imgCropped = crop(xmin, ymin, xmax, ymax, image_path)\n",
    "    image_info = ImageInfo(image=imgCropped,sub_class=sub_classes[i],value=\"\")\n",
    "    images_info_cropped.append(image_info)\n",
    "    if PLOTTING:\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(imgCropped)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2274dd56bfb7ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Handwriting Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32d50dd270d79",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Preprocess Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a886d9d68004c435",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:57.731927200Z",
     "start_time": "2024-02-03T12:28:57.494430100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "img_size=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "preprocessed_image_infos = []\n",
    "for image_info in images_info_cropped:\n",
    "    image = image_info.image \n",
    "    image = np.mean(image, axis=2, keepdims=True)\n",
    "    image = preprocess.distortion_free_resize(image, img_size)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    temp_sub_class = image_info.sub_class\n",
    "    temp_image_info = ImageInfo(image=image,sub_class=temp_sub_class,value=\"\")\n",
    "    preprocessed_image_infos.append(temp_image_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b587cee9abf935b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfbcac4daa071012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:57.736391700Z",
     "start_time": "2024-02-03T12:28:57.732930100Z"
    }
   },
   "outputs": [],
   "source": [
    "if PLOTTING:   \n",
    "    plot_image = preprocessed_image_infos[0].image\n",
    "    plot_image = np.transpose(plot_image, (1, 0, 2))\n",
    "    plot_image = np.flipud(plot_image)\n",
    "    plt.imshow(plot_image[:, :, 0],cmap='gray')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098d538d74bffb4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Handwriting Recognition Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6bf781cee4e3fd5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:57.747755200Z",
     "start_time": "2024-02-03T12:28:57.738400600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handwriting Model\n",
      "Loaded max_len: 65\n",
      "Loaded characters: ['[UNK]', '-', '.', '0', '1', '2', '4', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', 'ß', 'ä', 'ö', 'ü']\n",
      "Number Model\n",
      "Loaded number max_len: 15\n",
      "Loaded number characters: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "import pickle\n",
    "# Load from pickle file\n",
    "IAM = HANDWRITING_MODEL_NO == 1\n",
    "if IAM:\n",
    "    with open('iam_handwriting_model_characters.pkl', 'rb') as file:\n",
    "        loaded_max_len, loaded_characters = pickle.load(file)\n",
    "else:\n",
    "    with open('bafog_handwriting_model_characters.pkl', 'rb') as file:\n",
    "        loaded_max_len, loaded_characters = pickle.load(file)\n",
    "    \n",
    "with open('number_saved_max_len_char.pkl', 'rb') as file:\n",
    "     max_len_num, character_num = pickle.load(file)\n",
    "# Print loaded data\n",
    "print(\"Handwriting Model\")\n",
    "print(\"Loaded max_len:\", loaded_max_len)\n",
    "print(\"Loaded characters:\", loaded_characters)\n",
    "\n",
    "print(\"Number Model\")\n",
    "print(\"Loaded number max_len:\", max_len_num)\n",
    "print(\"Loaded number characters:\", character_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8a056aa12c2bc4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:28:57.779210400Z",
     "start_time": "2024-02-03T12:28:57.747755200Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import StringLookup\n",
    "char_to_num = StringLookup(vocabulary=list(loaded_characters), mask_token=None)\n",
    "num_to_char = StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "number_char_to_num = StringLookup(vocabulary=list(character_num), mask_token=None)\n",
    "number_num_to_char = StringLookup(vocabulary=number_char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :loaded_max_len]\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "def decode_number_pred(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :max_len_num]\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(number_num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text\n",
    "\n",
    "def load_model_numbers():\n",
    "    model_weight_path = \"models/only_numbers/only_numbers_weights.keras\"\n",
    "    model_path = \"models/only_numbers/\"\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model and weights...\")\n",
    "        model = load_model(model_path)\n",
    "        model.load_weights(model_weight_path)\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No pre-trained model or weights found.\")\n",
    "        return None\n",
    "def load_model_and_weights():\n",
    "    if IAM:\n",
    "        model_weight_path = \"models/model9v3_xl/model9v3_xl_weights.keras\"\n",
    "        model_path = \"models/model9v3_xl/\"\n",
    "    else:\n",
    "        model_weight_path = \"models/dense_and_full/transferlearningTestingModel_weights.keras\"\n",
    "        model_path = 'models/dense_and_full/'\n",
    "        # model_weight_path = \"models/denselayer1/transferlearningTestingModel_weights.keras\"\n",
    "        # model_path = 'models/denselayer1/'\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model and weights...\")\n",
    "        model = load_model(model_path)\n",
    "        model.load_weights(model_weight_path)\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No pre-trained model or weights found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd0422d0138b0426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:52.707547300Z",
     "start_time": "2024-02-03T12:28:57.768688900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/dense_and_full/\n",
      "Loading pre-trained model and weights...\n",
      "Model and weights loaded successfully.\n",
      "models/only_numbers/\n",
      "Loading pre-trained model and weights...\n",
      "Model and weights loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Handwriting\n",
    "handwriting_model = load_model_and_weights()\n",
    "prediction_model = keras.models.Model(handwriting_model.get_layer(name=\"image\").input, handwriting_model.get_layer(name=\"dense2\").output)\n",
    "\n",
    "number_model = load_model_numbers()\n",
    "number_pred_model = keras.models.Model(number_model.get_layer(name=\"image\").input, number_model.get_layer(name=\"dense2\").output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4255bf9e79b95c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spell Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dd6c439beb77c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:52.712528200Z",
     "start_time": "2024-02-03T12:29:52.709546800Z"
    }
   },
   "outputs": [],
   "source": [
    "def spell_checker(text):\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker(language='de')\n",
    "    words = [word for word in text.split(\" \") if word != '']\n",
    "    #Spellchecker\n",
    "    corrected_text = ' '.join([spell.correction(word) if spell.correction(word) is not None else word for word in text.split()])\n",
    "    return corrected_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106363bb404ff1cd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Map Sub_Classes to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d1ec31a7c3c4b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:52.721563300Z",
     "start_time": "2024-02-03T12:29:52.713528700Z"
    }
   },
   "outputs": [],
   "source": [
    "import bounding_box.config as bounding_box_config\n",
    "class_ids = bounding_box_config.class_ids\n",
    "\n",
    "def map_sub_class_to_string_and_sort(class_number):\n",
    "    temp_class_string = class_ids[class_number]\n",
    "    \n",
    "    not_class_list = [\"Ausbildung_Klasse\",\"Ausbildung\",\"Person\",\"Wohnsitz\",\"Wohnsitz_waehrend_Ausbildung\"]\n",
    "    if temp_class_string not in not_class_list:\n",
    "        return temp_class_string\n",
    "    return -1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95cad03f941a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4145288b4b15eaff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:58.767777100Z",
     "start_time": "2024-02-03T12:29:52.721563300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ja X Count: 55458 Nein X Count: 48780\n",
      "Ja X Count: 45063 Nein X Count: 47267\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 1s 573ms/step\n",
      "1/1 [==============================] - 1s 566ms/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "TEST\n",
      "(128, 1024) uint8\n",
      "(100, 100) uint8\n",
      "0.9288836121559143\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 1s 605ms/step\n",
      "1/1 [==============================] - 1s 581ms/step\n",
      "1/1 [==============================] - 1s 590ms/step\n",
      "1/1 [==============================] - 1s 590ms/step\n",
      "1/1 [==============================] - 1s 542ms/step\n",
      "1/1 [==============================] - 1s 914ms/step\n",
      "1/1 [==============================] - 1s 660ms/step\n",
      "1/1 [==============================] - 0s 488ms/step\n",
      "1/1 [==============================] - 0s 489ms/step\n",
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 1s 533ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 0s 485ms/step\n",
      "1/1 [==============================] - 1s 987ms/step\n",
      "1/1 [==============================] - 1s 503ms/step\n",
      "1/1 [==============================] - 1s 876ms/step\n",
      "1/1 [==============================] - 1s 624ms/step\n",
      "1/1 [==============================] - 1s 557ms/step\n",
      "1/1 [==============================] - 1s 792ms/step\n"
     ]
    }
   ],
   "source": [
    "images_with_value = []\n",
    "# Prediction\n",
    "for i, preprocess_image in enumerate(preprocessed_image_infos):\n",
    "    temp_sub_class = preprocess_image.sub_class\n",
    "    temp_sub_class_string = map_sub_class_to_string_and_sort(temp_sub_class)\n",
    "    number_sub_classes = ['Wohnsitz_waehrend_Ausbildung_Hausnummer','Wohnsitz_waehrend_Ausbildung_Postleitzahl','Wohnsitz_Hausnummer',\n",
    "                          'Wohnsitz_Postleitzahl','Person_Geburtsdatum','Person_Familienstand_seit','Ausbildung_Foerderungsnummer']\n",
    "    check_boxes_sub_classes =['Ausbildung_Antrag_gestellt_ja','Ausbildung_Vollzeit','Person_Kinder'] \n",
    "    \n",
    "    if temp_sub_class_string != -1:\n",
    "        if temp_sub_class_string in number_sub_classes:\n",
    "            preds = number_pred_model.predict(tf.expand_dims(preprocess_image.image, axis=0))\n",
    "            pred_texts = decode_number_pred(preds)\n",
    "            number_prediction = pred_texts[0]\n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=number_prediction)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        elif temp_sub_class_string in check_boxes_sub_classes:\n",
    "            import contrast_true_or_false.contrast_tof as check_box_checker\n",
    "            from PIL import Image\n",
    "            temp_preprocess_image = preprocess_image.image\n",
    "            temp_preprocess_image = np.transpose(temp_preprocess_image, (1, 0, 2))\n",
    "            temp_preprocess_image = np.flipud(temp_preprocess_image)\n",
    "            numpy_array = np.array(temp_preprocess_image)\n",
    "            numpy_array = numpy_array.squeeze()\n",
    "            numpy_array = np.clip(numpy_array, 0.0, 1.0)\n",
    "            image_array_uint8 = (numpy_array * 255).astype(np.uint8)\n",
    "            pil_image = Image.fromarray(image_array_uint8)\n",
    "            if temp_sub_class_string != \"Person_Kinder\":\n",
    "                result = check_box_checker.is_checkbox_checked(pil_image, PLOTTING)\n",
    "            else:\n",
    "                img_without_channel = np.squeeze(image_array_uint8)\n",
    "                \n",
    "                img_3_channels = cv2.merge([img_without_channel,img_without_channel,img_without_channel])\n",
    "                \n",
    "                result = check_box_checker.is_checkbox_checked_template(img_3_channels)\n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=result)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        else:\n",
    "            preds = prediction_model.predict(tf.expand_dims(preprocess_image.image, axis=0))\n",
    "            pred_texts = decode_batch_predictions(preds)\n",
    "            selected_pred_text = pred_texts[0]\n",
    "            selected_pred_text = selected_pred_text.replace(\"|\",\" \")\n",
    "            prediction_text = spell_checker(selected_pred_text)\n",
    "            temp_image_info = ImageInfo(image=preprocess_image.image,sub_class=temp_sub_class_string,value=prediction_text)\n",
    "            images_with_value.append(temp_image_info)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd322389744760",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot Predicted Text and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "417442424e90aef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:58.775708Z",
     "start_time": "2024-02-03T12:29:58.770777200Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_evaluation(images, rows=5, cols=None):\n",
    "    num_images = len(images)\n",
    "    if cols is None:\n",
    "        cols = -(-num_images // rows)  # Ceiling division to calculate the number of columns\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(25, 5))  # Adjust the figsize as needed\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        pred_texts = image.value\n",
    "        title = f\"{image.sub_class}: {pred_texts}\"\n",
    "        plot_image = image.image\n",
    "        \n",
    "        plot_image = np.transpose(plot_image, (1, 0, 2))\n",
    "        plot_image = np.flipud(plot_image)\n",
    "\n",
    "        # Calculate the position of the subplot in the grid\n",
    "        row_pos, col_pos = divmod(i, cols)\n",
    "\n",
    "        # Use the appropriate axis for each subplot\n",
    "        ax = axes[row_pos, col_pos] if num_images > 1 else axes\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.imshow(plot_image[:, :, 0], cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot parameters for better layout\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1aa8216da41b32f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:58.785745400Z",
     "start_time": "2024-02-03T12:29:58.774202100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausbildung_Antrag_gestellt_ja - Nein\n",
      "Ausbildung_Vollzeit - Ja\n",
      "Ausbilung_Abschluss - musste\n",
      "Ausbildung_Foerderungsnummer - 24003343077020\n",
      "Ausbildung_Staette - HochachueBonn\n",
      "Ausbildung_Amt - \n",
      "Person_Familienstand - \n",
      "Person_Kinder - Ja\n",
      "Person_Stattsangehörigkeit_Ehegatte - phi\n",
      "Person_Stattsangehörigkeit_eigene - DE\n",
      "Person_Familienstand_seit - 22272998\n",
      "Person_Geburtsdatum - 212002\n",
      "Person_Geburtsort - wie\n",
      "Person_Geburtsname - ein\n",
      "Person_Vorname - einer\n",
      "Person_Name - ihn\n",
      "Wohnsitz_Adresszusatz - es\n",
      "Wohnsitz_Land - DiE\n",
      "Wohnsitz_Ort - bahn\n",
      "Wohnsitz_Postleitzahl - 3\n",
      "Wohnsitz_Hausnummer - 22\n",
      "Wohnsitz_Strasse - hemmst\n",
      "Wohnsitz_waehrend_Ausbildung_Adresszusatz - los\n",
      "Wohnsitz_waehrend_Ausbildung_Land - wobei\n",
      "Wohnsitz_waehrend_Ausbildung_Ort - binnen\n",
      "Wohnsitz_waehrend_Ausbildung_Postleitzahl - 42389\n",
      "Wohnsitz_waehrend_Ausbildung_Hausnummer - 22\n",
      "Wohnsitz_waehrend_Ausbildung_Strasse - leimen\n"
     ]
    }
   ],
   "source": [
    "if PLOTTING:\n",
    "    plot_evaluation(images_with_value)\n",
    "else:\n",
    "    for image in images_with_value:\n",
    "        print(f\"{image.sub_class} - {image.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3562eeda11c90c0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90d0fa2f04982044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T12:29:58.809226700Z",
     "start_time": "2024-02-03T12:29:58.784239500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.33496117591858\n"
     ]
    }
   ],
   "source": [
    "print(time.time() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95664aeebf3aaa3f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import utils.configs as Config\n",
    "config_path = \"utils/configs.json\"\n",
    "config = Config.Config(config_path)\n",
    "\n",
    "\n",
    "SHOULD_TRAIN = bool(config.get_pipeline_parameter()[\"training\"])\n",
    "SHOULD_PLOT = bool(config.get_pipeline_parameter()[\"plotting\"])\n",
    "BB_MODEL_NO = config.get_pipeline_parameter()[\"bb_model\"]\n",
    "HW_MODEL_NO = config.get_pipeline_parameter()[\"handwriting_model\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T15:41:04.501162800Z",
     "start_time": "2024-01-07T15:41:04.446269400Z"
    }
   },
   "id": "95c2e25903326405",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "importing the required packages and libraries."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95eaf4da49f14168"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-07T15:41:18.463168300Z",
     "start_time": "2024-01-07T15:41:05.693083500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\timBo\\miniconda3\\envs\\Pipeline-Smartapp\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow import keras\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddde87861bb7c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Globale Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64569232484acbf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'bounding_box/workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "ORIGINAL_ANNOTATION_PATH = ANNOTATION_PATH+'/original'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "ORIGINAL_IMAGE_PATH = IMAGE_PATH+'/original'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "TEST_IMAGE_PATH = IMAGE_PATH +'/Bilder'\n",
    " \n",
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "NEW_HEIGHT = 640\n",
    "NEW_WIDTH = 640  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6913913a7c628",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0ef724d142aa2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "\"Ausbildung\",\n",
    "\"Ausbildung_Klasse\" ,\n",
    "\"Ausbildung_Antrag_gestellt_ja\" ,\n",
    "\"Ausbildung_Antrag_gestellt_nein\" ,\n",
    "\"Ausbildung_Amt\" ,\n",
    "\"Ausbildung_Foerderungsnummer\" ,\n",
    "\"Ausbilung_Abschluss\" ,\n",
    "\"Ausbildung_Vollzeit\" ,\n",
    "\"Ausbildung\" ,\n",
    "\"Ausbildung_Teilzeit\" ,\n",
    "\"Ausbildung_Staette\" ,\n",
    "\"Person\" ,\n",
    "\"Person_Geburtsort\" ,\n",
    "\"Person_maennlich\" ,\n",
    "\"Person_Geburtsdatum\" ,\n",
    "\"Person_weiblich\",\n",
    "\"Person_divers\",\n",
    "\"Person_Name\",\n",
    "\"Person_Familienstand\" ,\n",
    "\"Person_Vorname\" ,\n",
    "\"Person_Geburtsname\" ,\n",
    "\"Person_Familienstand_seit\",\n",
    "\"Person_Stattsangehörigkeit_eigene\" ,\n",
    "\"Person_Stattsangehörigkeit_Ehegatte\" ,\n",
    "\"Person_Kinder\",\n",
    "\"Wohnsitz_Strasse\",\n",
    "\"Wohnsitz_Land\",\n",
    "\"Wohnsitz_Postleitzahl\",\n",
    "\"Wohnsitz\",\n",
    "\"Wohnsitz_Hausnummer\",\n",
    "\"Wohnsitz_Adresszusatz\",\n",
    "\"Wohnsitz_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung\" ,\n",
    "\"Wohnsitz_waehrend_Ausbildung_Strasse\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Hausnummer\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Land\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_nein\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Adresszusatz\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Postleitzahl\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_ja\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete_nein\"\n",
    "]\n",
    "sub_class_ids = [\n",
    "\"Ausbildung_Staette\" ,\n",
    "\"Ausbildung_Klasse\" ,\n",
    "\"Ausbildung_Antrag_gestellt_ja\" ,\n",
    "\"Ausbildung_Antrag_gestellt_nein\" ,\n",
    "\"Ausbildung_Amt\" ,\n",
    "\"Ausbildung_Foerderungsnummer\" ,\n",
    "\"Ausbilung_Abschluss\" ,\n",
    "\"Ausbildung_Vollzeit\" ,\n",
    "\"Ausbildung\" ,\n",
    "\"Ausbildung_Teilzeit\" ,\n",
    "\"Person_Geburtsort\" ,\n",
    "\"Person_maennlich\" ,\n",
    "\"Person_Geburtsdatum\" ,\n",
    "\"Person_weiblich\",\n",
    "\"Person_divers\",\n",
    "\"Person_Name\",\n",
    "\"Person_Familienstand\" ,\n",
    "\"Person_Vorname\" ,\n",
    "\"Person_Geburtsname\" ,\n",
    "\"Person_Familienstand_seit\",\n",
    "\"Person_Stattsangehörigkeit_eigene\" ,\n",
    "\"Person_Stattsangehörigkeit_Ehegatte\" ,\n",
    "\"Person_Kinder\",\n",
    "\"Wohnsitz_Strasse\",\n",
    "\"Wohnsitz_Land\",\n",
    "\"Wohnsitz_Postleitzahl\",\n",
    "\"Wohnsitz_Hausnummer\",\n",
    "\"Wohnsitz_Adresszusatz\",\n",
    "\"Wohnsitz_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Strasse\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Hausnummer\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Land\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_nein\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Adresszusatz\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Postleitzahl\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_ja\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete_nein\"\n",
    "]\n",
    "main_class_ids=[\n",
    "    \"Wohnsitz_waehrend_Ausbildung\" ,\n",
    "    \"Ausbildung\",\n",
    "    \"Person\" ,\n",
    "    \"Wohnsitz\",\n",
    "]\n",
    "\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "main_class_mapping = dict(zip(range(len(main_class_ids)), main_class_ids))\n",
    "sub_class_mapping = dict(zip(range(len(sub_class_ids)), sub_class_ids))\n",
    "print(class_mapping)\n",
    "print(main_class_mapping)\n",
    "print(sub_class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6758ef1fa42299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(ORIGINAL_ANNOTATION_PATH, file_name)\n",
    "        for file_name in os.listdir(ORIGINAL_ANNOTATION_PATH)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(ORIGINAL_IMAGE_PATH, file_name)\n",
    "        for file_name in os.listdir(ORIGINAL_IMAGE_PATH)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b54ce7ddf2f5ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "parsing the XML annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class_id(classes, cls):    \n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return class_ids\n",
    "\n",
    "def create_box(bbox):\n",
    "    xmin = float(bbox.find(\"xmin\").text)\n",
    "    ymin = float(bbox.find(\"ymin\").text)\n",
    "    xmax = float(bbox.find(\"xmax\").text)\n",
    "    ymax = float(bbox.find(\"ymax\").text)\n",
    "    return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b0dfa61d8a842",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    " \n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(ORIGINAL_IMAGE_PATH, image_name)\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    main_classes = []\n",
    "    sub_classes = []\n",
    "    main_boxes = []\n",
    "    sub_boxes = []\n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    " \n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        boxes.append( create_box(bbox))\n",
    "        #main labels\n",
    "        if (cls=='Wohnsitz_waehrend_Ausbildung')or(cls=='Ausbildung') or (cls =='Person') or (cls == 'Wohnsitz'):\n",
    "            main_classes.append(cls)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            main_boxes.append( create_box(bbox))\n",
    "        else:\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            sub_boxes.append( create_box(bbox))\n",
    "            sub_classes.append(cls)\n",
    "            print(cls)\n",
    "    class_ids = map_class_id(classes, cls)\n",
    "    main_class_ids = map_class_id(main_classes, cls)            \n",
    "    sub_class_ids = map_class_id(sub_classes, cls)\n",
    "\n",
    "    return image_path, boxes, class_ids, main_class_ids, sub_class_ids, main_boxes, sub_boxes, image_name\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd7af4378f4935",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "image_names = []\n",
    "bbox = []\n",
    "classes = []\n",
    "main_classes = []\n",
    "sub_classes= []\n",
    "main_bbox= []\n",
    "sub_bbox= []\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids, main_class_ids,sub_class_ids, main_boxes, sub_boxes, image_name = parse_annotation(xml_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)\n",
    "    image_names.append(image_name)\n",
    "\n",
    "    main_classes.append(main_class_ids)\n",
    "    sub_classes.append(sub_class_ids)\n",
    "    main_bbox.append(main_boxes)\n",
    "    sub_bbox.append(sub_boxes)\n",
    "\n",
    "arr=np.array(sub_bbox)\n",
    "print(arr.shape)\n",
    "subset_image_paths_all=image_paths\n",
    "subset_class_ids_all=sub_classes\n",
    "subset_boxes_all=sub_bbox\n",
    "print(subset_class_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_imgaes(input_path, output_path, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "    resized_image = cv2.resize(image, (width, height))\n",
    "    cv2.imwrite(output_path, resized_image)    \n",
    "    \n",
    "def scale_bounding_boxes(input_path,bounding_boxes, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "\n",
    "    height_ratio = height / image.shape[0]\n",
    "    width_ratio = width / image.shape[1]\n",
    "    resized_boxes_for_one_image=[]\n",
    "    #for box in bounding_boxes:\n",
    "    resized_boxes = []\n",
    "    x_min = (bounding_boxes[0]*width_ratio)\n",
    "    y_min = (bounding_boxes[1]*height_ratio)\n",
    "    x_max = (bounding_boxes[2]*width_ratio)\n",
    "    y_max = (bounding_boxes[3]*height_ratio)\n",
    "    #resized_boxes.append([x_min,y_min,x_max,y_max])\n",
    "    #resized_boxes_for_one_image.append(resized_boxes)\n",
    "    return [x_min,y_min,x_max,y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Images from main_classes\n",
    "IMAGES_MAIN_CLASSES_PATH = IMAGE_PATH +'/640x640_main_classes'\n",
    "\n",
    "def create_main_images(input_path, output_path, box):\n",
    "    startY, endY, startX, endX = int(np.round(box[1])), int(np.round(box[3])), int(np.round(box[0])), int(np.round(box[2]))\n",
    "    image =  cv2.imread(input_path)\n",
    "    cropped_image = image[startY:endY, startX:endX]\n",
    "    cv2.imwrite(output_path, cropped_image)  \n",
    "\n",
    "box_ratio_factors = []\n",
    "sub_box_calculated = []\n",
    "main_images_paths = []\n",
    "main_image_names = []\n",
    "\n",
    "def calculate_box_ratio_factor(image_path,box):\n",
    "    image = cv2.imread(image_path)\n",
    "    width = image.shape[0]\n",
    "    height = image.shape[1]\n",
    "    box[2], box[2], box[2], box[2]\n",
    "    relative_xbl = box[0] #/ width\n",
    "    relative_ybl = box[1] #/ height\n",
    "    relative_xtr = box[2] #/ width\n",
    "    relative_ytr = box[3] #/ height\n",
    "    return [relative_xbl,relative_ybl,relative_xtr,relative_ytr]\n",
    "\n",
    "def array_calculation(sub_box, main_box):\n",
    "    return [sub_box[0]-main_box[0],sub_box[1]-main_box[1],sub_box[2]-main_box[0],sub_box[3]-main_box[1]]\n",
    "\n",
    "#Bilder ausschneiden und speichern\n",
    "for i in range(len(main_class_ids)):\n",
    "    for j in range(len(image_paths)):\n",
    "        box_ratio_factors.append(calculate_box_ratio_factor(image_paths[j], main_bbox[j][i]))\n",
    "        create_main_images(image_paths[j],IMAGES_MAIN_CLASSES_PATH+'/'+str(i)+'_'+str(j)+'.jpg', main_bbox[j][i])\n",
    "        main_images_paths.append(IMAGES_MAIN_CLASSES_PATH+'/'+str(i)+'_'+str(j)+'.jpg')\n",
    "        main_image_names.append(str(i)+'_'+str(j)+'.jpg')\n",
    "\n",
    "#Resize Images auf 640x640\n",
    "subsset_scaled_image_paths = []\n",
    "for img in range(len(main_images_paths)):\n",
    "    resize_imgaes(main_images_paths[img], IMAGE_PATH+ '/640x640_main_classes_scaled/' +main_image_names[img], 640, 640)\n",
    "    subsset_scaled_image_paths.append(IMAGE_PATH+ '/640x640_main_classes_scaled/' +main_image_names[img])\n",
    "#print(subsset_scaled_image_paths)\n",
    "\n",
    "#Anpassen der Bounding Box Koordinaten zu dem jeweiligen Bildausschnitt\n",
    "for i in range(len(main_class_ids)):\n",
    "    for j in range(len(image_paths)):\n",
    "        for k in range(len(sub_class_ids)):\n",
    "            sub_box_calculated.append(array_calculation(sub_bbox[j][k], main_bbox[j][i]))\n",
    "\n",
    "\n",
    "\n",
    "sub_boxes_main_class_1=[]  \n",
    "sub_boxes_main_class_2=[] \n",
    "sub_boxes_main_class_3=[] \n",
    "sub_boxes_main_class_4=[] \n",
    "sub_boxes_all_main_classes=[]\n",
    "\n",
    "scaled_sub_boxes_main_class_1=[]  \n",
    "scaled_sub_boxes_main_class_2=[] \n",
    "scaled_sub_boxes_main_class_3=[] \n",
    "scaled_sub_boxes_main_class_4=[] \n",
    "\n",
    "#Boxen im Verhältnis der main_box zu 640x640 scalen\n",
    "for i in range(len(image_names)): #18 Bilder\n",
    "    for j in range(len(sub_class_ids)): #38 Klassen\n",
    "        arr=[]\n",
    "        if j < 10: #Ausbildung\n",
    "            sub_boxes_main_class_1.append(array_calculation(sub_bbox[i][j], main_bbox[i][0]))\n",
    "            scaled_sub_boxes_main_class_1.append(scale_bounding_boxes(main_images_paths[i],sub_boxes_main_class_1[i],640,640))\n",
    "\n",
    "        elif (j >= 10) and (j <= 21) :#Person\n",
    "            sub_boxes_main_class_2.append(array_calculation(sub_bbox[i][j], main_bbox[i][1]))\n",
    "            scaled_sub_boxes_main_class_2.append(scale_bounding_boxes(main_images_paths[i+18],sub_boxes_main_class_2[i],640,640))\n",
    "        elif (j >= 22) and (j <= 27) :#Wohnsitz\n",
    "            #sub_boxes_main_class_3.append(sub_bbox[i][j])\n",
    "            sub_boxes_main_class_3.append(array_calculation(sub_bbox[i][j], main_bbox[i][2]))\n",
    "            scaled_sub_boxes_main_class_3.append(scale_bounding_boxes(main_images_paths[i+36],sub_boxes_main_class_3[i],640,640))\n",
    "        elif (j >= 28) and (j <= 37) :#Wohnsitz_während_Ausildung\n",
    "            sub_boxes_main_class_4.append(array_calculation(sub_bbox[i][j], main_bbox[i][3]))\n",
    "            scaled_sub_boxes_main_class_4.append(scale_bounding_boxes(main_images_paths[i+54],sub_boxes_main_class_4[i],640,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33321ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, boxes):\n",
    "    image = cv2.imread(image)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "    # Bounding Boxes zeichnen\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = map(int, box)\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(main_images_paths[0],sub_boxes_main_class_1)\n",
    "show_image(main_images_paths[18],sub_boxes_main_class_2)\n",
    "show_image(main_images_paths[36],sub_boxes_main_class_3)\n",
    "show_image(main_images_paths[54],sub_boxes_main_class_4)\n",
    "\n",
    "show_image(subsset_scaled_image_paths[0],scaled_sub_boxes_main_class_1)\n",
    "show_image(subsset_scaled_image_paths[18],scaled_sub_boxes_main_class_2)\n",
    "show_image(subsset_scaled_image_paths[36],scaled_sub_boxes_main_class_3)\n",
    "show_image(subsset_scaled_image_paths[54],scaled_sub_boxes_main_class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 4 datasets\n",
    "#image_paths\n",
    "#shape(18,x)\n",
    "subset_images_paths_1 = []\n",
    "subset_images_paths_2 = []\n",
    "subset_images_paths_3 = []\n",
    "subset_images_paths_4 = []\n",
    "for i in range(len(subsset_scaled_image_paths)):\n",
    "        if i < 18: #Ausbildung\n",
    "            subset_images_paths_1.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 18) and (i <= 35) :#Person\n",
    "            subset_images_paths_2.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 36) and (i <= 53) :#Wohnsitz\n",
    "            subset_images_paths_3.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 54) and (i <= 73) :#Wohnsitz_während_Ausildung\n",
    "            subset_images_paths_4.append(subsset_scaled_image_paths[i])\n",
    "#classes\n",
    "subset_class_ids_1_1 = []\n",
    "subset_class_ids_2_2 = []\n",
    "subset_class_ids_3_3 = []\n",
    "subset_class_ids_4_4 = []\n",
    "subset_class_ids_1 = []\n",
    "subset_class_ids_2 = []\n",
    "subset_class_ids_3 = []\n",
    "subset_class_ids_4 = []\n",
    "classes=[]\n",
    "#shape(18,x)\n",
    "for i in range(len(subset_class_ids_all[0])):\n",
    "        if i < 10: #Ausbildung\n",
    "                subset_class_ids_1_1.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 10) and (i <= 21) :#Person\n",
    "                subset_class_ids_2_2.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 22) and (i <= 27) :#Wohnsitz\n",
    "                subset_class_ids_3_3.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 28) and (i <= 37) :#Wohnsitz_während_Ausildung\n",
    "                subset_class_ids_4_4.append(subset_class_ids_all[0][i])\n",
    "\n",
    "for i in range (0,18):\n",
    "    subset_class_ids_1.append(subset_class_ids_1_1)\n",
    "    subset_class_ids_2.append(subset_class_ids_2_2)\n",
    "    subset_class_ids_3.append(subset_class_ids_3_3)\n",
    "    subset_class_ids_4.append(subset_class_ids_4_4)\n",
    "\n",
    "#boxen\n",
    "subset_boxes_1 = []\n",
    "subset_boxes_2 = []\n",
    "subset_boxes_3 = []\n",
    "subset_boxes_4 = []\n",
    "#shape(18,x,4)\n",
    "\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_1)\n",
    "arr_1=arr.reshape(18,10,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,10):\n",
    "        arr_1_1.append(arr_1[i][0])\n",
    "    subset_boxes_1.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_2)\n",
    "arr_2=arr.reshape(18,12,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,12):\n",
    "        arr_1_1.append(arr_2[i][0])\n",
    "    subset_boxes_2.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_3)\n",
    "arr_3=arr.reshape(18,6,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,6):\n",
    "        arr_1_1.append(arr_3[i][0])\n",
    "    subset_boxes_3.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_4)\n",
    "arr_4=arr.reshape(18,10,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,10):\n",
    "        arr_1_1.append(arr_4[i][0])\n",
    "    subset_boxes_4.append(arr_1_1)\n",
    "\n",
    "print(subset_class_ids_1)\n",
    "arr=np.array(subset_class_ids_4)\n",
    "print(arr.shape)\n",
    "arrs=np.array(subset_class_ids_1)\n",
    "print(arrs.shape)\n",
    "arrf=np.array(subset_boxes_1)\n",
    "print(arrf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conc_paths = np.concatenate((subset_images_paths_1, subset_images_paths_2, subset_images_paths_3, subset_images_paths_4), axis=0)\n",
    "#print(conc_paths.shape)\n",
    "#conc_boxes = np.concatenate((subset_boxes_1, subset_boxes_2, subset_boxes_3, subset_boxes_4), axis=1)\n",
    "#print(conc_boxes.shape)\n",
    "#conc_classes = np.concatenate(( subset_class_ids_1, subset_class_ids_2, subset_class_ids_3, subset_class_ids_4), axis=1)\n",
    "#print(conc_classes.shape)\n",
    "\n",
    "classes=[]\n",
    "images=[]\n",
    "box=[]\n",
    "#shape (72,) subset aus 4 main_boxen\n",
    "for i in range(len(subset_boxes_1)):\n",
    "    box.append(subset_boxes_1[i])\n",
    "    images.append(subset_images_paths_1[i])\n",
    "    classes.append(subset_class_ids_1[i])\n",
    "for i in range(len(subset_boxes_2)):\n",
    "    box.append(subset_boxes_2[i])\n",
    "    images.append(subset_images_paths_2[i])\n",
    "    classes.append(subset_class_ids_2[i])\n",
    "for i in range(len(subset_boxes_3)):\n",
    "    box.append(subset_boxes_3[i])\n",
    "    images.append(subset_images_paths_3[i])\n",
    "    classes.append(subset_class_ids_3[i])\n",
    "for i in range(len(subset_boxes_4)):\n",
    "    box.append(subset_boxes_4[i])\n",
    "    images.append(subset_images_paths_4[i])\n",
    "    classes.append(subset_class_ids_4[i])\n",
    "\n",
    "#hinzugügen des sets der ganzen Seite\n",
    "for i in range(len(subset_boxes_all)):\n",
    "    box.append(subset_boxes_all[i])\n",
    "    images.append(subset_image_paths_all[i])\n",
    "    classes.append(subset_class_ids_all[i])\n",
    "\n",
    "arr=np.array(subset_boxes_1)\n",
    "print(arr.shape)\n",
    "print(len(classes))\n",
    "print(classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_tensor(image_paths,classes,bbox):\n",
    "    bbox = tf.ragged.constant(bbox, dtype=tf.float32)\n",
    "    classes = tf.ragged.constant(classes, dtype=tf.int64)\n",
    "    image_paths = tf.ragged.constant(image_paths, dtype=tf.string)\n",
    "    data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed64ac0efe7e20",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merged_sub_data = create_data_tensor(merged_paths, merged_classes, merged_boxes)\n",
    "\n",
    "new_sub_data_all = create_data_tensor(images,classes, box )\n",
    "\n",
    "#new_sub_data_all = create_data_tensor(subset_image_paths_all, subset_class_ids_all, subset_boxes_all)\n",
    "\n",
    "new_sub_data_1 = create_data_tensor(subset_images_paths_1, subset_class_ids_1, subset_boxes_1)\n",
    "new_sub_data_2 = create_data_tensor(subset_images_paths_2, subset_class_ids_2, subset_boxes_2)\n",
    "new_sub_data_3 = create_data_tensor(subset_images_paths_3, subset_class_ids_3, subset_boxes_3)\n",
    "new_sub_data_4 = create_data_tensor(subset_images_paths_4, subset_class_ids_4, subset_boxes_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a780ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(split, data_tensor):\n",
    "    num_val = int(split * SPLIT_RATIO)#int(len(image_path_list)\n",
    "    val_data = data_tensor.take(num_val)\n",
    "    train_data = data_tensor.skip(num_val)\n",
    "    return val_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bilder durchmischen\n",
    "new_sub_data_all = new_sub_data_all.shuffle(90, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_val_data_all,sub_train_data_all  = create_train_test_data(18 , zipped_ds)\n",
    "\n",
    "\n",
    "sub_val_data_all, sub_train_data_all  = create_train_test_data(90 , new_sub_data_all)\n",
    "\n",
    "sub_val_data_1, sub_train_data_1  = create_train_test_data(18 , new_sub_data_1)\n",
    "sub_val_data_2, sub_train_data_2  = create_train_test_data(18 , new_sub_data_2)\n",
    "sub_val_data_3, sub_train_data_3  = create_train_test_data(18 , new_sub_data_3)\n",
    "sub_val_data_4, sub_train_data_4  = create_train_test_data(18 , new_sub_data_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59876dac6b8109d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    " \n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbff8fae67f4f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        #keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        #keras_cv.layers.RandomRotation(factor=0.2, bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640),\n",
    "            scale_factor=(1, 1),\n",
    "            bounding_box_format=\"xyxy\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_ds(train_data):\n",
    "    train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(BATCH_SIZE * 8)\n",
    "    train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sub_train_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_ds_all = create_train_ds(sub_train_data_all)\n",
    "\n",
    "sub_train_ds_1 = create_train_ds(sub_train_data_1)\n",
    "sub_train_ds_2 = create_train_ds(sub_train_data_2)\n",
    "sub_train_ds_3 = create_train_ds(sub_train_data_3)\n",
    "sub_train_ds_4 = create_train_ds(sub_train_data_4)\n",
    "\n",
    "#sub_trains_ds_zipped = tf.data.Dataset.zip(sub_train_ds_all, sub_train_ds_1, sub_train_ds_2, sub_train_ds_3, sub_train_ds_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c4d13d07a9021",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(1, 1),\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "def create_val_ds(val_data):\n",
    "    val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.shuffle(BATCH_SIZE * 8)\n",
    "    val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_val_ds_all= create_train_ds(sub_val_data_all)\n",
    "\n",
    "sub_val_ds_1 = create_train_ds(sub_val_data_1)\n",
    "sub_val_ds_2 = create_train_ds(sub_val_data_2)\n",
    "sub_val_ds_3 = create_train_ds(sub_val_data_3)\n",
    "sub_val_ds_4 = create_train_ds(sub_val_data_4)\n",
    "\n",
    "#sub_val_ds_zipped = tf.data.Dataset.zip(sub_val_ds_all, sub_val_ds_1, sub_val_ds_2, sub_val_ds_3, sub_val_ds_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dfd9ee0435599",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.2,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(sub_train_ds_all, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=1, cols=2)\n",
    "visualize_dataset(sub_val_ds_all, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=1, cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed721cc85379c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "def create_train_val_tuple(train_ds,val_ds):\n",
    "    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_ds_all,sub_val_ds_all=create_train_val_tuple(sub_train_ds_all,sub_val_ds_all)\n",
    "sub_train_ds_1,sub_val_ds_1=create_train_val_tuple(sub_train_ds_1,sub_val_ds_1)\n",
    "sub_train_ds_2,sub_val_ds_2=create_train_val_tuple(sub_train_ds_2,sub_val_ds_2)\n",
    "sub_train_ds_3,sub_val_ds_3=create_train_val_tuple(sub_train_ds_3,sub_val_ds_3)\n",
    "sub_train_ds_4,sub_val_ds_4=create_train_val_tuple(sub_train_ds_4,sub_val_ds_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e170d82fe24bd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aa6f584e4c14f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_backbone(backbone):\n",
    "    backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "        \"yolo_v8_xs_backbone_coco\",\n",
    "         load_weights=True \n",
    "    )\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96ab8f16010f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df5fe5c59a00aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_optimizer():\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        global_clipnorm=GLOBAL_CLIPNORM, #This ensures that gradients, which influence the model’s parameter updates, \n",
    "                                        # don’t become exceedingly large and destabilize training.\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd6e9a7ee37a17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define YOLO8Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5f43561951061",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_model(num_classes):\n",
    "    model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=num_classes, #the number of object classes\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=define_backbone(\"yolo_v8_xs_backbone_coco\"),\n",
    "    fpn_depth=1,\n",
    ")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75549b0bd6aa448",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71693c31705379",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(\n",
    "    optimizer=define_optimizer(), \n",
    "    classification_loss=\"binary_crossentropy\", #calculates the discrepancy between anticipated class probabilities and actual class probabilities\n",
    "    box_loss=\"ciou\" # box_loss -> measure the difference between the predicted bounding boxes and the ground truth\n",
    "                    # he Complete IoU (CIoU) metric is used, which not only measures the overlap between predicted and ground truth bounding \n",
    "                    # boxes but also considers the difference in aspect ratio, center distance, and box size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02b3a86101acca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc14bcd28e72b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss',  \n",
    "                               patience=3,          \n",
    "                               restore_best_weights=True)  \n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "def fit_model(model, train_data, validation_data):\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=EPOCH,\n",
    "        callbacks=early_stopping,\n",
    "        #callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"workspace/models/yolo_coco_model.h5\")],\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d3178697d79ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define NonMaxSuppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d60b0d51b9293",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_NonMaxSuppression(model):\n",
    "    model.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "        bounding_box_format=\"xyxy\",\n",
    "        from_logits=True,\n",
    "        iou_threshold=0.9,\n",
    "        confidence_threshold=0.5\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3681e4f8e05b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define a base model, compile the base model and then loads the weights from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882e067f7329069",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_weight_model(model_path):\n",
    "    base_model = define_model(len(class_mapping))\n",
    "    compile_model(base_model)\n",
    "    loaded_model = base_model.load_weights(model_path)\n",
    "    return  loaded_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d40400ff2d63b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save weights of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47feba0eebb6d985",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_weights(model, name):\n",
    "    save_model_path = MODEL_PATH + name\n",
    "    model.save_weights(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0c1e8d8d2b27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define visualization methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2516f2d228dcce8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format, class_mapping):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=5,\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        show=True,\n",
    "        font_scale=0.4,\n",
    "        class_mapping= class_mapping ,#class_mapping,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3796fc7867b4fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define and compile model for sub_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04f885e2c1b782",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_coco_sub = define_model(43)\n",
    "compile_model(yolo_coco_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218a7e8c569fabd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define and compile model for main_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66bf3db6b01bc7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_coco_main = define_model(len(main_class_mapping))\n",
    "compile_model(yolo_coco_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_MODEL_PATH = MODEL_PATH+'/main_bbox_detector_model.h5'\n",
    "\n",
    "#yolo_coco_main.load_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1769fff44ecc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit main_bbox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851752000ef9c7a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main_bbox_history = fit_model(yolo_coco_main, main_train_ds, main_val_ds )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0665310e4b897",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit sub_bbox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d130532e0fbb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yolo_coco_sub.fit(sub_trains_ds_zipped, epochs=5, validation_data=sub_val_ds_zipped)\n",
    "\n",
    "sub_bbox_history_all= fit_model(yolo_coco_sub,sub_train_ds_all,sub_val_ds_all )\n",
    "\n",
    "NEWSUB_5_SETS_MODEL_PATH = MODEL_PATH+'/new_sub_bbox_detector_model_5_datasets.h5'\n",
    "#yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "#yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "#sub_bbox_history_1 = fit_model(yolo_coco_sub, sub_train_ds_1, sub_val_ds_1 )\n",
    "\"\"\"\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_2 = fit_model(yolo_coco_sub, sub_train_ds_2, sub_val_ds_2 )\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_3 = fit_model(yolo_coco_sub, sub_train_ds_3, sub_val_ds_3 )\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_4 = fit_model(yolo_coco_sub, sub_train_ds_4, sub_val_ds_4 )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529b5fb",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d999d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Box Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['box_loss'], label='Training Box Loss')\n",
    "    plt.plot(history.history['val_box_loss'], label='Validation Box Loss')\n",
    "    plt.title('Training and Validation Box Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Box Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Klassen Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['class_loss'], label='Training Class Loss')\n",
    "    plt.plot(history.history['val_class_loss'], label='Validation Class Loss')\n",
    "    plt.title('Training and Validation Class Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Class Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6406faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(sub_bbox_history_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb0a670477657e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualization main bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a74fd92642989",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize_detections(yolo_coco_main, dataset=main_val_ds, bounding_box_format=\"xyxy\", class_mapping= main_class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc464b76f7c4b71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualization sub bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d209caeefc4f547",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_detections(yolo_coco_sub, dataset=sub_val_ds_all, bounding_box_format=\"xyxy\", class_mapping= class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd044daa5edc01b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b9e1271bfded2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAIN_MODEL_PATH = MODEL_PATH+'/main_bbox_detector_model.h5'\n",
    "\n",
    "#yolo_coco_main.save_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ad9bcda410868",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "load model for sub_bbox and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88f862ef2d6244",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yolo_coco_sub.load_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a8f31b3f9f3c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualize a prediction with loaded_sub_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_image(image_path, model):\n",
    "    image =  cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (640, 640))\n",
    "    resized_image = np.expand_dims(resized_image, axis=0)  \n",
    "\n",
    "    #predict\n",
    "    predictions = model.predict(resized_image)\n",
    "    boxes = predictions['boxes']\n",
    "    confidence = predictions['confidence']\n",
    "    classes = predictions['classes']\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for box, conf, cls in zip(boxes[0], confidence[0], classes[0]):\n",
    "        if conf > 0.1:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            #label = f\"Class {cls} ({conf:.2f})\"\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')#, label=label)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaac925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_image(image_path, model):\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (640, 640))\n",
    "    resized_image = np.expand_dims(resized_image, axis=0)  \n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(resized_image)\n",
    "    boxes = predictions['boxes']\n",
    "    confidence = predictions['confidence']\n",
    "    classes = predictions['classes']\n",
    "\n",
    "    sorted_indices = np.argsort(-confidence[0])  \n",
    "    boxes = boxes[0][sorted_indices]\n",
    "    confidence = confidence[0][sorted_indices]\n",
    "    classes = classes[0][sorted_indices]\n",
    "\n",
    "    unique_classes = np.unique(classes)\n",
    "    selected_indices = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(classes == cls)[0]\n",
    "        if len(cls_indices) > 0:\n",
    "            best_idx = cls_indices[0]  \n",
    "            selected_indices.append(best_idx)\n",
    "\n",
    "    selected_boxes = boxes[selected_indices]\n",
    "    selected_confidence = confidence[selected_indices]\n",
    "    selected_classes = classes[selected_indices]\n",
    "\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for box, conf, cls in zip(selected_boxes, selected_confidence, selected_classes):\n",
    "        if conf > 0.1:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            label = f\"Class {cls} ({conf:.2f})\"\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none' ,label=label)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lade bild\n",
    "image_path1 = (IMAGE_PATH+ '/640x640_main_classes_scaled/0_4.jpg') \n",
    "image_path2 = (ORIGINAL_IMAGE_PATH+'/image_0004.jpg') \n",
    "predict_on_image(image_path1, yolo_coco_sub)\n",
    "predict_on_image(image_path2, yolo_coco_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d43ee844c1840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path)  # Save the model when mAP improves\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict Images Bounding Box"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b83f47b37997236a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de25d596d2429a23",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Crop ROI"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d73327eb722f2ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Crop ROI\n",
    "import cv2\n",
    "def crop(xmin, ymin, xmax, ymax, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    xmin = int(round(xmin))\n",
    "    ymin = int(round(ymin))\n",
    "    xmax = int(round(xmax))\n",
    "    ymax = int(round(ymax))\n",
    "    # width = int(round(width))\n",
    "    # height = int(round(height))\n",
    "    # rowBeg = y\n",
    "    # rowEnd = y + height\n",
    "    # columnBeg = x\n",
    "    # columnEnd = x + width\n",
    "    imgCropped = image[ymin:ymax, xmin:xmax]\n",
    "    return imgCropped\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "save_directory = 'data'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "images_cropped = []\n",
    "boxes = boxes[0]\n",
    "for box in boxes:\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    imgCropped = crop(xmin, ymin, xmax, ymax, IMAGE_PATH)\n",
    "    images_cropped.append(imgCropped)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(imgCropped, cmap='gray')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dbc953849a6b899",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Handwriting"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdc51121e5bd01e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Unsere Klassen\n",
    "import handwriting.load_data as load_data\n",
    "import handwriting.preprocess as preprocess\n",
    "import handwriting.testing_models as testing_models # Use: build_model9v3(img_width, img_height, char) \n",
    "import utils.configs as Config\n",
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "617741a46be8f4e3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d912de093664402a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config_path = \"utils/configs.json\"\n",
    "config = Config.Config(config_path)\n",
    "\n",
    "# Model Parameter\n",
    "MODEL_SAVE = bool(config.get_model_parameter()[\"save\"])\n",
    "MODEL_NAME = config.get_model_parameter()[\"name\"]\n",
    "IMAGE_WIDTH = config.get_model_parameter()[\"width\"] # default: 1024\n",
    "IMAGE_HEIGHT = config.get_model_parameter()[\"height\"] # default: 64\n",
    "\n",
    "# Directory Parameter\n",
    "MODEL_DIR_NAME = pathlib.Path(os.getcwd()).joinpath(config.get_directory_parameter()[\"model_dir\"])\n",
    "TEST_RESULT_DIR_NAME = pathlib.Path(os.getcwd()).joinpath(config.get_directory_parameter()[\"test_dir\"])\n",
    "DATA_BASE_PATH = config.get_directory_parameter()[\"data_base_path\"]\n",
    "\n",
    "# Training Parameter\n",
    "SAVE_HISTORY = bool(config.get_training_parameter()[\"save_history\"])\n",
    "EPOCHS = config.get_training_parameter()[\"epochs\"]\n",
    "BATCH_SIZE = config.get_training_parameter()[\"batch_size\"] # default: 32 - 48\n",
    "TF_SEED = config.get_training_parameter()[\"tf_seed\"] # default: 42\n",
    "LEARNING_RATE = config.get_training_parameter()[\"learning_rate\"]\n",
    "PATIENCE = config.get_training_parameter()[\"patience\"] # default: 3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89cafc2f961d6b8d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d50ebf19d620853"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Default: seed = 42\n",
    "# np.random.seed(TF_SEED)\n",
    "# tf.random.set_seed(TF_SEED)\n",
    "\n",
    "# Random\n",
    "#seed = random.randint(1, 1000)\n",
    "#np.random.seed(seed)\n",
    "#tf.random.set_seed(seed)\n",
    "print(LEARNING_RATE)\n",
    "print(TF_SEED)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78be0da09e6b20c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "load_data.print_samples(DATA_BASE_PATH)\n",
    "if False: # IAM Dataset\n",
    "    x_train_img_paths, y_train_labels = load_data.get_train_data()\n",
    "    x_val_img_paths, y_val_labels = load_data.get_validation_data()\n",
    "    #x_test_img_paths, y_test_labels = load_data.get_test_data()\n",
    "    \n",
    "#TODO\n",
    "# need  x_train_img_paths, y_train_labels =\n",
    "#       x_val_img_paths, y_val_labels =\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31e96e3bacfefa84",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(f\"Training path: {x_train_img_paths[0:2]}\", y_train_labels[0:2])\n",
    "print(f\"Validation path: {x_val_img_paths[0:2]}\", y_val_labels[0:2])\n",
    "#print(f\"Testing path: {x_test_img_paths[0:2]}\", y_test_labels[0:2])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7bc94188289acd0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15d396ce20431c83"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Has to be here because load data functions need to be called before\n",
    "import handwriting.tokenizer as tokenizer\n",
    "import handwriting.custom_image_generator as cgi\n",
    "\n",
    "# takes eternity\n",
    "#x_train, y_train = tokenizer.prepare_data(x_train_img_paths, y_train_labels) \n",
    "#x_test, y_test = tokenizer.prepare_data(x_test_img_paths, y_test_labels)\n",
    "\n",
    "#train_generator = cgi.CustomImageGenerator(x_train_img_paths, y_train_labels, BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(x_train_img_paths, y_train_labels, (IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "val_ds = tokenizer.prepare_dataset(x_val_img_paths, y_val_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "#test_ds = tokenizer.prepare_dataset(x_test_img_paths, y_test_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "#aug_train_ds = tokenizer.prepare_augmented_dataset(x_train_img_paths, y_train_labels, BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c49c72e5da9634ba",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Show Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5267aa06119792c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "    ax = plt.subplots(4, 4, figsize=(32, 8))[1]\n",
    "\n",
    "    for i in range(16):\n",
    "        img = images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        # Gather indices where label!= padding_token.\n",
    "        label = labels[i]\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, tokenizer.padding_token)))\n",
    "        # Convert to string.\n",
    "        label = tf.strings.reduce_join(tokenizer.num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1843d431baf541",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Augmentation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a37a927a1a58b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To see the augmentations from CustomImageGenerator\n",
    "train_generator = cgi.CustomImageGenerator(x_train_img_paths, y_train_labels, tokenizer.batch_size, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "example_batch = train_generator[0]\n",
    "augmented_images = example_batch[0]['image']\n",
    "\n",
    "num_to_plot = 4\n",
    "fig, axes = plt.subplots(1, num_to_plot, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(np.squeeze(augmented_images[i]), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86d0b142ed5b54cf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomBrightness(0.5,value_range=(0, 1), seed=TF_SEED),\n",
    "        tf.keras.layers.RandomContrast(0.5,seed=TF_SEED)\n",
    "    ]\n",
    ")\n",
    "\n",
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(images[0].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "# Apply data augmentation to the image\n",
    "augmented_images = data_augmentation(images, training=True)\n",
    "\n",
    "# Display the augmented images\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 4, i + 2)\n",
    "    plt.imshow(augmented_images[i].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(f\"Augmented Image {i+1}\")\n",
    "    \n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322476a450778bc6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Keras Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de342f6f643d2f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "weights_keras_string =\"_weights.keras\"\n",
    "\n",
    "def model_load_weights_if_exists(model):\n",
    "    MODEL_MODEL_PATH = MODEL_NAME\n",
    "    MODEL_WEIGHT_PATH = MODEL_NAME + weights_keras_string\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, MODEL_MODEL_PATH)\n",
    "    model_weight_path = os.path.join(model_path, MODEL_WEIGHT_PATH)\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Resuming Training where we left off!\")\n",
    "        model.load_weights(model_weight_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de9c9f0a326f83ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    model_load_weights_if_exists(model)\n",
    "        \n",
    "    prediction_model = keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "    # checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE * 0.2, min_lr=1e-6, verbose=1)\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[reduce_lr, early_stopping])    \n",
    "    return prediction_model, history"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "678df9fcdf15f347",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Default\n",
    "# model = testing_models.build_model_default(IMAGE_WIDTH, IMAGE_HEIGHT, char, MODEL_NAME)\n",
    "\n",
    "char = len(tokenizer.char_to_num.get_vocabulary())\n",
    "model = testing_models.build_model9v3_random(IMAGE_WIDTH, IMAGE_HEIGHT, char, LEARNING_RATE)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ef162ef12836ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "prediction_model, history = train_model(model)\n",
    "\n",
    "total_duration = time.time() - start_time\n",
    "print(\"Gesamte Trainingsdauer: {time}s\".format(time=round(total_duration)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d9ce7165e15a145",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot helper functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc27d01fe397042"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_history(history, name, dir_path, save_fig):\n",
    "    \"\"\"\n",
    "    Plottet die Historie des Trainings eines Models und speichert die in einem Verzeichnis ab \n",
    "\n",
    "    :param history: Das trainierte Modell\n",
    "    :param name: Name, wie das Modell gespeicht werden soll\n",
    "    :param name: Verzeichniss, wo der Plot gespeichert weren soll\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    metrics = history.history\n",
    "    _, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot für Trainings- und Validierungsverluste\n",
    "    ax1.plot(metrics['loss'], label='Training Loss', color='blue')\n",
    "    ax1.plot(metrics['val_loss'], label='Validation Loss', color='red')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='black')\n",
    "    ax1.tick_params('y', colors='black')\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(0.0, 0.95))  \n",
    "\n",
    "    # Zweite Y-Achse für die Lernrate\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(metrics['lr'], label='Learning Rate', color='green')\n",
    "    ax2.set_ylabel('Learning Rate', color='black')\n",
    "    \n",
    "    ax2.set_yscale('log')  # Verwende logarithmische Skala für die Lernrate\n",
    "    \n",
    "    ax2.tick_params('y', colors='black')\n",
    "    ax2.yaxis.set_major_formatter(StrMethodFormatter('{x:1.0e}'))\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))  \n",
    "    \n",
    "    if save_fig:\n",
    "        plt.title('Name: '+name)\n",
    "        path = os.path.join(dir_path, name + '_history.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e443bfc89b1a8fea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_dir(path_to_dir):\n",
    "    isExist = os.path.exists(path_to_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(path_to_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2d89800205cdc0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# A utility function to decode the output of the network.\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :load_data.max_len]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(tokenizer.num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21c285abdfcc81c8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_evaluation(name, dir_path, save_fig):\n",
    "    for batch in test_ds.take(1):\n",
    "        batch_images = batch[\"image\"]\n",
    "        _, ax = plt.subplots(4, 4, figsize=(32, 8))\n",
    "\n",
    "        preds = prediction_model.predict(batch_images)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "        for i in range(16):\n",
    "            img = batch_images[i]\n",
    "            img = tf.image.flip_left_right(img)\n",
    "            img = tf.transpose(img, perm=[1, 0, 2])\n",
    "            img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "            img = img[:, :, 0]\n",
    "\n",
    "            title = f\"Prediction: {pred_texts[i]}\"\n",
    "            ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "            ax[i // 4, i % 4].set_title(title)\n",
    "            ax[i // 4, i % 4].axis(\"off\")   \n",
    "    if save_fig:\n",
    "        path = os.path.join(dir_path, name + '_result.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e2678f21bfbbb9",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46158f62a4aac633"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_new_plot_name(model_name, names, format):\n",
    "    import re\n",
    "    pattern = r\"\\d+\"\n",
    "    max_number = 0\n",
    "    for name in names:\n",
    "        tmp_name = name.replace(model_name,\"\")\n",
    "        number = int(re.findall(pattern,tmp_name)[0])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "            \n",
    "    new_model_name = model_name + \"V_\" + str(max_number + 1)\n",
    "    return format.replace(model_name,new_model_name)\n",
    "        \n",
    "if not os.path.exists(TEST_RESULT_DIR_NAME):\n",
    "            create_dir(TEST_RESULT_DIR_NAME)\n",
    "files_with_model_name = [file for file in os.listdir(TEST_RESULT_DIR_NAME) if MODEL_NAME in file]\n",
    "metrics = history.history\n",
    "\n",
    "NAME = \"{name}_{epoch}E_{height}H_{width}W_{loss}L_{val_loss}VL_{time}s\".format(\n",
    "    name=MODEL_NAME, epoch=history.epoch[-1], height=IMAGE_HEIGHT, width=IMAGE_WIDTH,\n",
    "    loss=round(metrics['loss'][-1],2), val_loss=round(metrics['val_loss'][-1], 2), time=round(total_duration))\n",
    "\n",
    "if not files_with_model_name:\n",
    "    if SAVE_HISTORY:\n",
    "        plot_history(history, NAME, TEST_RESULT_DIR_NAME, True)\n",
    "        plot_evaluation(NAME, TEST_RESULT_DIR_NAME, True)\n",
    "else:\n",
    "    new_name = create_new_plot_name(MODEL_NAME,files_with_model_name, NAME)\n",
    "    plot_history(history, new_name, TEST_RESULT_DIR_NAME, True)\n",
    "    plot_evaluation(new_name, TEST_RESULT_DIR_NAME, True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d958dca138b01f54",
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "source": [
    "# Save the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e38b9c0b0ad1538"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if MODEL_SAVE:\n",
    "    if not os.path.exists(MODEL_DIR_NAME):\n",
    "        create_dir(MODEL_DIR_NAME)\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, \"{model_name}\".format(model_name=MODEL_NAME))\n",
    "    model.save(model_path)\n",
    "    model.save_weights(os.path.join(model_path, f\"{MODEL_NAME}{weights_keras_string}\"), overwrite=True, save_format=None, options=None)\n",
    "    json_string = model.to_json()\n",
    "\n",
    "    with open(os.path.join(model_path, f\"{MODEL_NAME}.json\"),'w') as f:\n",
    "        f.write(json_string)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fde914d352e2192",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c1bdeface7466e95"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28498a62598e4bb8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

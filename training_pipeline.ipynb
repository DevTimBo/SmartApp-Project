{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95664aeebf3aaa3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2e25903326405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:34:01.885153300Z",
     "start_time": "2024-01-09T13:34:01.861292800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils.configs as Config\n",
    "config_path = \"utils/configs.json\"\n",
    "config = Config.Config(config_path)\n",
    "\n",
    "\n",
    "SHOULD_TRAIN = bool(config.get_pipeline_parameter()[\"training\"])\n",
    "SHOULD_PLOT = bool(config.get_pipeline_parameter()[\"plotting\"])\n",
    "BB_MODEL_NO = config.get_pipeline_parameter()[\"bb_model\"]\n",
    "HW_MODEL_NO = config.get_pipeline_parameter()[\"handwriting_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eaf4da49f14168",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "importing the required packages and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:34:06.873841600Z",
     "start_time": "2024-01-09T13:34:02.759731100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow import keras\n",
    "from keras_cv import bounding_box\n",
    "from keras_cv import visualization\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddde87861bb7c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Globale Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64569232484acbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:34:08.353084900Z",
     "start_time": "2024-01-09T13:34:08.337523400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'bounding_box/workspace'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "ORIGINAL_ANNOTATION_PATH = ANNOTATION_PATH+'/original'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "ORIGINAL_IMAGE_PATH = IMAGE_PATH+'/original'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "TEST_IMAGE_PATH = IMAGE_PATH +'/Bilder'\n",
    " \n",
    "SPLIT_RATIO = 0.2\n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCH = 5\n",
    "GLOBAL_CLIPNORM = 10.0\n",
    "NEW_HEIGHT = 640\n",
    "NEW_WIDTH = 640  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6913913a7c628",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0ef724d142aa2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_ids = [\n",
    "\"Ausbildung\",\n",
    "\"Ausbildung_Klasse\" ,\n",
    "\"Ausbildung_Antrag_gestellt_ja\" ,\n",
    "\"Ausbildung_Antrag_gestellt_nein\" ,\n",
    "\"Ausbildung_Amt\" ,\n",
    "\"Ausbildung_Foerderungsnummer\" ,\n",
    "\"Ausbilung_Abschluss\" ,\n",
    "\"Ausbildung_Vollzeit\" ,\n",
    "\"Ausbildung\" ,\n",
    "\"Ausbildung_Teilzeit\" ,\n",
    "\"Ausbildung_Staette\" ,\n",
    "\"Person\" ,\n",
    "\"Person_Geburtsort\" ,\n",
    "\"Person_maennlich\" ,\n",
    "\"Person_Geburtsdatum\" ,\n",
    "\"Person_weiblich\",\n",
    "\"Person_divers\",\n",
    "\"Person_Name\",\n",
    "\"Person_Familienstand\" ,\n",
    "\"Person_Vorname\" ,\n",
    "\"Person_Geburtsname\" ,\n",
    "\"Person_Familienstand_seit\",\n",
    "\"Person_Stattsangehörigkeit_eigene\" ,\n",
    "\"Person_Stattsangehörigkeit_Ehegatte\" ,\n",
    "\"Person_Kinder\",\n",
    "\"Wohnsitz_Strasse\",\n",
    "\"Wohnsitz_Land\",\n",
    "\"Wohnsitz_Postleitzahl\",\n",
    "\"Wohnsitz\",\n",
    "\"Wohnsitz_Hausnummer\",\n",
    "\"Wohnsitz_Adresszusatz\",\n",
    "\"Wohnsitz_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung\" ,\n",
    "\"Wohnsitz_waehrend_Ausbildung_Strasse\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Hausnummer\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Land\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_nein\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Adresszusatz\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Postleitzahl\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_ja\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete_nein\"\n",
    "]\n",
    "sub_class_ids = [\n",
    "\"Ausbildung_Staette\" ,\n",
    "\"Ausbildung_Klasse\" ,\n",
    "\"Ausbildung_Antrag_gestellt_ja\" ,\n",
    "\"Ausbildung_Antrag_gestellt_nein\" ,\n",
    "\"Ausbildung_Amt\" ,\n",
    "\"Ausbildung_Foerderungsnummer\" ,\n",
    "\"Ausbilung_Abschluss\" ,\n",
    "\"Ausbildung_Vollzeit\" ,\n",
    "\"Ausbildung\" ,\n",
    "\"Ausbildung_Teilzeit\" ,\n",
    "\"Person_Geburtsort\" ,\n",
    "\"Person_maennlich\" ,\n",
    "\"Person_Geburtsdatum\" ,\n",
    "\"Person_weiblich\",\n",
    "\"Person_divers\",\n",
    "\"Person_Name\",\n",
    "\"Person_Familienstand\" ,\n",
    "\"Person_Vorname\" ,\n",
    "\"Person_Geburtsname\" ,\n",
    "\"Person_Familienstand_seit\",\n",
    "\"Person_Stattsangehörigkeit_eigene\" ,\n",
    "\"Person_Stattsangehörigkeit_Ehegatte\" ,\n",
    "\"Person_Kinder\",\n",
    "\"Wohnsitz_Strasse\",\n",
    "\"Wohnsitz_Land\",\n",
    "\"Wohnsitz_Postleitzahl\",\n",
    "\"Wohnsitz_Hausnummer\",\n",
    "\"Wohnsitz_Adresszusatz\",\n",
    "\"Wohnsitz_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Strasse\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Hausnummer\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Land\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Ort\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_nein\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Adresszusatz\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_Postleitzahl\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternwohnung_ja\",\n",
    "\"Wohnsitz_waehrend_Ausbildung_elternmiete_nein\"\n",
    "]\n",
    "main_class_ids=[\n",
    "    \"Wohnsitz_waehrend_Ausbildung\" ,\n",
    "    \"Ausbildung\",\n",
    "    \"Person\" ,\n",
    "    \"Wohnsitz\",\n",
    "]\n",
    "\n",
    "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n",
    "main_class_mapping = dict(zip(range(len(main_class_ids)), main_class_ids))\n",
    "sub_class_mapping = dict(zip(range(len(sub_class_ids)), sub_class_ids))\n",
    "print(class_mapping)\n",
    "print(main_class_mapping)\n",
    "print(sub_class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6758ef1fa42299",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(ORIGINAL_ANNOTATION_PATH, file_name)\n",
    "        for file_name in os.listdir(ORIGINAL_ANNOTATION_PATH)\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(ORIGINAL_IMAGE_PATH, file_name)\n",
    "        for file_name in os.listdir(ORIGINAL_IMAGE_PATH)\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b54ce7ddf2f5ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "parsing the XML annotation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_class_id(classes, cls):    \n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return class_ids\n",
    "\n",
    "def create_box(bbox):\n",
    "    xmin = float(bbox.find(\"xmin\").text)\n",
    "    ymin = float(bbox.find(\"ymin\").text)\n",
    "    xmax = float(bbox.find(\"xmax\").text)\n",
    "    ymax = float(bbox.find(\"ymax\").text)\n",
    "    return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b0dfa61d8a842",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_annotation(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    " \n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = os.path.join(ORIGINAL_IMAGE_PATH, image_name)\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    main_classes = []\n",
    "    sub_classes = []\n",
    "    main_boxes = []\n",
    "    sub_boxes = []\n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    " \n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        boxes.append( create_box(bbox))\n",
    "        #main labels\n",
    "        if (cls=='Wohnsitz_waehrend_Ausbildung')or(cls=='Ausbildung') or (cls =='Person') or (cls == 'Wohnsitz'):\n",
    "            main_classes.append(cls)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            main_boxes.append( create_box(bbox))\n",
    "        else:\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            sub_boxes.append( create_box(bbox))\n",
    "            sub_classes.append(cls)\n",
    "            print(cls)\n",
    "    class_ids = map_class_id(classes, cls)\n",
    "    main_class_ids = map_class_id(main_classes, cls)            \n",
    "    sub_class_ids = map_class_id(sub_classes, cls)\n",
    "\n",
    "    return image_path, boxes, class_ids, main_class_ids, sub_class_ids, main_boxes, sub_boxes, image_name\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd7af4378f4935",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "image_names = []\n",
    "bbox = []\n",
    "classes = []\n",
    "main_classes = []\n",
    "sub_classes= []\n",
    "main_bbox= []\n",
    "sub_bbox= []\n",
    "for xml_file in tqdm(xml_files):\n",
    "    image_path, boxes, class_ids, main_class_ids,sub_class_ids, main_boxes, sub_boxes, image_name = parse_annotation(xml_file)\n",
    "    image_paths.append(image_path)\n",
    "    bbox.append(boxes)\n",
    "    classes.append(class_ids)\n",
    "    image_names.append(image_name)\n",
    "\n",
    "    main_classes.append(main_class_ids)\n",
    "    sub_classes.append(sub_class_ids)\n",
    "    main_bbox.append(main_boxes)\n",
    "    sub_bbox.append(sub_boxes)\n",
    "\n",
    "arr=np.array(sub_bbox)\n",
    "print(arr.shape)\n",
    "subset_image_paths_all=image_paths\n",
    "subset_class_ids_all=sub_classes\n",
    "subset_boxes_all=sub_bbox\n",
    "print(subset_class_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d91e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_imgaes(input_path, output_path, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "    resized_image = cv2.resize(image, (width, height))\n",
    "    cv2.imwrite(output_path, resized_image)    \n",
    "    \n",
    "def scale_bounding_boxes(input_path,bounding_boxes, height, width):\n",
    "    image =  cv2.imread(input_path)\n",
    "\n",
    "    height_ratio = height / image.shape[0]\n",
    "    width_ratio = width / image.shape[1]\n",
    "    resized_boxes_for_one_image=[]\n",
    "    #for box in bounding_boxes:\n",
    "    resized_boxes = []\n",
    "    x_min = (bounding_boxes[0]*width_ratio)\n",
    "    y_min = (bounding_boxes[1]*height_ratio)\n",
    "    x_max = (bounding_boxes[2]*width_ratio)\n",
    "    y_max = (bounding_boxes[3]*height_ratio)\n",
    "    #resized_boxes.append([x_min,y_min,x_max,y_max])\n",
    "    #resized_boxes_for_one_image.append(resized_boxes)\n",
    "    return [x_min,y_min,x_max,y_max]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc7bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Images from main_classes\n",
    "IMAGES_MAIN_CLASSES_PATH = IMAGE_PATH +'/640x640_main_classes'\n",
    "\n",
    "def create_main_images(input_path, output_path, box):\n",
    "    startY, endY, startX, endX = int(np.round(box[1])), int(np.round(box[3])), int(np.round(box[0])), int(np.round(box[2]))\n",
    "    image =  cv2.imread(input_path)\n",
    "    cropped_image = image[startY:endY, startX:endX]\n",
    "    cv2.imwrite(output_path, cropped_image)  \n",
    "\n",
    "box_ratio_factors = []\n",
    "sub_box_calculated = []\n",
    "main_images_paths = []\n",
    "main_image_names = []\n",
    "\n",
    "def calculate_box_ratio_factor(image_path,box):\n",
    "    image = cv2.imread(image_path)\n",
    "    width = image.shape[0]\n",
    "    height = image.shape[1]\n",
    "    box[2], box[2], box[2], box[2]\n",
    "    relative_xbl = box[0] #/ width\n",
    "    relative_ybl = box[1] #/ height\n",
    "    relative_xtr = box[2] #/ width\n",
    "    relative_ytr = box[3] #/ height\n",
    "    return [relative_xbl,relative_ybl,relative_xtr,relative_ytr]\n",
    "\n",
    "def array_calculation(sub_box, main_box):\n",
    "    return [sub_box[0]-main_box[0],sub_box[1]-main_box[1],sub_box[2]-main_box[0],sub_box[3]-main_box[1]]\n",
    "\n",
    "#Bilder ausschneiden und speichern\n",
    "for i in range(len(main_class_ids)):\n",
    "    for j in range(len(image_paths)):\n",
    "        box_ratio_factors.append(calculate_box_ratio_factor(image_paths[j], main_bbox[j][i]))\n",
    "        create_main_images(image_paths[j],IMAGES_MAIN_CLASSES_PATH+'/'+str(i)+'_'+str(j)+'.jpg', main_bbox[j][i])\n",
    "        main_images_paths.append(IMAGES_MAIN_CLASSES_PATH+'/'+str(i)+'_'+str(j)+'.jpg')\n",
    "        main_image_names.append(str(i)+'_'+str(j)+'.jpg')\n",
    "\n",
    "#Resize Images auf 640x640\n",
    "subsset_scaled_image_paths = []\n",
    "for img in range(len(main_images_paths)):\n",
    "    resize_imgaes(main_images_paths[img], IMAGE_PATH+ '/640x640_main_classes_scaled/' +main_image_names[img], 640, 640)\n",
    "    subsset_scaled_image_paths.append(IMAGE_PATH+ '/640x640_main_classes_scaled/' +main_image_names[img])\n",
    "#print(subsset_scaled_image_paths)\n",
    "\n",
    "#Anpassen der Bounding Box Koordinaten zu dem jeweiligen Bildausschnitt\n",
    "for i in range(len(main_class_ids)):\n",
    "    for j in range(len(image_paths)):\n",
    "        for k in range(len(sub_class_ids)):\n",
    "            sub_box_calculated.append(array_calculation(sub_bbox[j][k], main_bbox[j][i]))\n",
    "\n",
    "\n",
    "\n",
    "sub_boxes_main_class_1=[]  \n",
    "sub_boxes_main_class_2=[] \n",
    "sub_boxes_main_class_3=[] \n",
    "sub_boxes_main_class_4=[] \n",
    "sub_boxes_all_main_classes=[]\n",
    "\n",
    "scaled_sub_boxes_main_class_1=[]  \n",
    "scaled_sub_boxes_main_class_2=[] \n",
    "scaled_sub_boxes_main_class_3=[] \n",
    "scaled_sub_boxes_main_class_4=[] \n",
    "\n",
    "#Boxen im Verhältnis der main_box zu 640x640 scalen\n",
    "for i in range(len(image_names)): #18 Bilder\n",
    "    for j in range(len(sub_class_ids)): #38 Klassen\n",
    "        arr=[]\n",
    "        if j < 10: #Ausbildung\n",
    "            sub_boxes_main_class_1.append(array_calculation(sub_bbox[i][j], main_bbox[i][0]))\n",
    "            scaled_sub_boxes_main_class_1.append(scale_bounding_boxes(main_images_paths[i],sub_boxes_main_class_1[i],640,640))\n",
    "\n",
    "        elif (j >= 10) and (j <= 21) :#Person\n",
    "            sub_boxes_main_class_2.append(array_calculation(sub_bbox[i][j], main_bbox[i][1]))\n",
    "            scaled_sub_boxes_main_class_2.append(scale_bounding_boxes(main_images_paths[i+18],sub_boxes_main_class_2[i],640,640))\n",
    "        elif (j >= 22) and (j <= 27) :#Wohnsitz\n",
    "            #sub_boxes_main_class_3.append(sub_bbox[i][j])\n",
    "            sub_boxes_main_class_3.append(array_calculation(sub_bbox[i][j], main_bbox[i][2]))\n",
    "            scaled_sub_boxes_main_class_3.append(scale_bounding_boxes(main_images_paths[i+36],sub_boxes_main_class_3[i],640,640))\n",
    "        elif (j >= 28) and (j <= 37) :#Wohnsitz_während_Ausildung\n",
    "            sub_boxes_main_class_4.append(array_calculation(sub_bbox[i][j], main_bbox[i][3]))\n",
    "            scaled_sub_boxes_main_class_4.append(scale_bounding_boxes(main_images_paths[i+54],sub_boxes_main_class_4[i],640,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33321ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, boxes):\n",
    "    image = cv2.imread(image)\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) \n",
    "\n",
    "    # Bounding Boxes zeichnen\n",
    "    for box in boxes:\n",
    "        xmin, ymin, xmax, ymax = map(int, box)\n",
    "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image(main_images_paths[0],sub_boxes_main_class_1)\n",
    "show_image(main_images_paths[18],sub_boxes_main_class_2)\n",
    "show_image(main_images_paths[36],sub_boxes_main_class_3)\n",
    "show_image(main_images_paths[54],sub_boxes_main_class_4)\n",
    "\n",
    "show_image(subsset_scaled_image_paths[0],scaled_sub_boxes_main_class_1)\n",
    "show_image(subsset_scaled_image_paths[18],scaled_sub_boxes_main_class_2)\n",
    "show_image(subsset_scaled_image_paths[36],scaled_sub_boxes_main_class_3)\n",
    "show_image(subsset_scaled_image_paths[54],scaled_sub_boxes_main_class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 4 datasets\n",
    "#image_paths\n",
    "#shape(18,x)\n",
    "subset_images_paths_1 = []\n",
    "subset_images_paths_2 = []\n",
    "subset_images_paths_3 = []\n",
    "subset_images_paths_4 = []\n",
    "for i in range(len(subsset_scaled_image_paths)):\n",
    "        if i < 18: #Ausbildung\n",
    "            subset_images_paths_1.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 18) and (i <= 35) :#Person\n",
    "            subset_images_paths_2.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 36) and (i <= 53) :#Wohnsitz\n",
    "            subset_images_paths_3.append(subsset_scaled_image_paths[i])\n",
    "        elif (i >= 54) and (i <= 73) :#Wohnsitz_während_Ausildung\n",
    "            subset_images_paths_4.append(subsset_scaled_image_paths[i])\n",
    "#classes\n",
    "subset_class_ids_1_1 = []\n",
    "subset_class_ids_2_2 = []\n",
    "subset_class_ids_3_3 = []\n",
    "subset_class_ids_4_4 = []\n",
    "subset_class_ids_1 = []\n",
    "subset_class_ids_2 = []\n",
    "subset_class_ids_3 = []\n",
    "subset_class_ids_4 = []\n",
    "classes=[]\n",
    "#shape(18,x)\n",
    "for i in range(len(subset_class_ids_all[0])):\n",
    "        if i < 10: #Ausbildung\n",
    "                subset_class_ids_1_1.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 10) and (i <= 21) :#Person\n",
    "                subset_class_ids_2_2.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 22) and (i <= 27) :#Wohnsitz\n",
    "                subset_class_ids_3_3.append(subset_class_ids_all[0][i])\n",
    "        elif (i >= 28) and (i <= 37) :#Wohnsitz_während_Ausildung\n",
    "                subset_class_ids_4_4.append(subset_class_ids_all[0][i])\n",
    "\n",
    "for i in range (0,18):\n",
    "    subset_class_ids_1.append(subset_class_ids_1_1)\n",
    "    subset_class_ids_2.append(subset_class_ids_2_2)\n",
    "    subset_class_ids_3.append(subset_class_ids_3_3)\n",
    "    subset_class_ids_4.append(subset_class_ids_4_4)\n",
    "\n",
    "#boxen\n",
    "subset_boxes_1 = []\n",
    "subset_boxes_2 = []\n",
    "subset_boxes_3 = []\n",
    "subset_boxes_4 = []\n",
    "#shape(18,x,4)\n",
    "\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_1)\n",
    "arr_1=arr.reshape(18,10,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,10):\n",
    "        arr_1_1.append(arr_1[i][0])\n",
    "    subset_boxes_1.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_2)\n",
    "arr_2=arr.reshape(18,12,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,12):\n",
    "        arr_1_1.append(arr_2[i][0])\n",
    "    subset_boxes_2.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_3)\n",
    "arr_3=arr.reshape(18,6,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,6):\n",
    "        arr_1_1.append(arr_3[i][0])\n",
    "    subset_boxes_3.append(arr_1_1)\n",
    "\n",
    "arr= np.array(scaled_sub_boxes_main_class_4)\n",
    "arr_4=arr.reshape(18,10,4)\n",
    "for j in range(0,18):\n",
    "    arr_1_1=[]\n",
    "    for i in range(0,10):\n",
    "        arr_1_1.append(arr_4[i][0])\n",
    "    subset_boxes_4.append(arr_1_1)\n",
    "\n",
    "print(subset_class_ids_1)\n",
    "arr=np.array(subset_class_ids_4)\n",
    "print(arr.shape)\n",
    "arrs=np.array(subset_class_ids_1)\n",
    "print(arrs.shape)\n",
    "arrf=np.array(subset_boxes_1)\n",
    "print(arrf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conc_paths = np.concatenate((subset_images_paths_1, subset_images_paths_2, subset_images_paths_3, subset_images_paths_4), axis=0)\n",
    "#print(conc_paths.shape)\n",
    "#conc_boxes = np.concatenate((subset_boxes_1, subset_boxes_2, subset_boxes_3, subset_boxes_4), axis=1)\n",
    "#print(conc_boxes.shape)\n",
    "#conc_classes = np.concatenate(( subset_class_ids_1, subset_class_ids_2, subset_class_ids_3, subset_class_ids_4), axis=1)\n",
    "#print(conc_classes.shape)\n",
    "\n",
    "classes=[]\n",
    "images=[]\n",
    "box=[]\n",
    "#shape (72,) subset aus 4 main_boxen\n",
    "for i in range(len(subset_boxes_1)):\n",
    "    box.append(subset_boxes_1[i])\n",
    "    images.append(subset_images_paths_1[i])\n",
    "    classes.append(subset_class_ids_1[i])\n",
    "for i in range(len(subset_boxes_2)):\n",
    "    box.append(subset_boxes_2[i])\n",
    "    images.append(subset_images_paths_2[i])\n",
    "    classes.append(subset_class_ids_2[i])\n",
    "for i in range(len(subset_boxes_3)):\n",
    "    box.append(subset_boxes_3[i])\n",
    "    images.append(subset_images_paths_3[i])\n",
    "    classes.append(subset_class_ids_3[i])\n",
    "for i in range(len(subset_boxes_4)):\n",
    "    box.append(subset_boxes_4[i])\n",
    "    images.append(subset_images_paths_4[i])\n",
    "    classes.append(subset_class_ids_4[i])\n",
    "\n",
    "#hinzugügen des sets der ganzen Seite\n",
    "for i in range(len(subset_boxes_all)):\n",
    "    box.append(subset_boxes_all[i])\n",
    "    images.append(subset_image_paths_all[i])\n",
    "    classes.append(subset_class_ids_all[i])\n",
    "\n",
    "arr=np.array(subset_boxes_1)\n",
    "print(arr.shape)\n",
    "print(len(classes))\n",
    "print(classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_tensor(image_paths,classes,bbox):\n",
    "    bbox = tf.ragged.constant(bbox, dtype=tf.float32)\n",
    "    classes = tf.ragged.constant(classes, dtype=tf.int64)\n",
    "    image_paths = tf.ragged.constant(image_paths, dtype=tf.string)\n",
    "    data = tf.data.Dataset.from_tensor_slices((image_paths, classes, bbox))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ed64ac0efe7e20",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merged_sub_data = create_data_tensor(merged_paths, merged_classes, merged_boxes)\n",
    "\n",
    "new_sub_data_all = create_data_tensor(images,classes, box )\n",
    "\n",
    "#new_sub_data_all = create_data_tensor(subset_image_paths_all, subset_class_ids_all, subset_boxes_all)\n",
    "\n",
    "new_sub_data_1 = create_data_tensor(subset_images_paths_1, subset_class_ids_1, subset_boxes_1)\n",
    "new_sub_data_2 = create_data_tensor(subset_images_paths_2, subset_class_ids_2, subset_boxes_2)\n",
    "new_sub_data_3 = create_data_tensor(subset_images_paths_3, subset_class_ids_3, subset_boxes_3)\n",
    "new_sub_data_4 = create_data_tensor(subset_images_paths_4, subset_class_ids_4, subset_boxes_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a780ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_data(split, data_tensor):\n",
    "    num_val = int(split * SPLIT_RATIO)#int(len(image_path_list)\n",
    "    val_data = data_tensor.take(num_val)\n",
    "    train_data = data_tensor.skip(num_val)\n",
    "    return val_data, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bilder durchmischen\n",
    "new_sub_data_all = new_sub_data_all.shuffle(90, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_val_data_all,sub_train_data_all  = create_train_test_data(18 , zipped_ds)\n",
    "\n",
    "\n",
    "sub_val_data_all, sub_train_data_all  = create_train_test_data(90 , new_sub_data_all)\n",
    "\n",
    "sub_val_data_1, sub_train_data_1  = create_train_test_data(18 , new_sub_data_1)\n",
    "sub_val_data_2, sub_train_data_2  = create_train_test_data(18 , new_sub_data_2)\n",
    "sub_val_data_3, sub_train_data_3  = create_train_test_data(18 , new_sub_data_3)\n",
    "sub_val_data_4, sub_train_data_4  = create_train_test_data(18 , new_sub_data_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59876dac6b8109d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    return image\n",
    " \n",
    "\n",
    "def load_dataset(image_path, classes, bbox):\n",
    "    # Read Image\n",
    "    image = load_image(image_path)\n",
    "    bounding_boxes = {\n",
    "        \"classes\": tf.cast(classes, dtype=tf.float32),\n",
    "        \"boxes\": bbox,\n",
    "    }\n",
    "    return {\"images\": tf.cast(image, tf.float32), \"bounding_boxes\": bounding_boxes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fbff8fae67f4f5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "augmenter = keras.Sequential(\n",
    "    layers=[\n",
    "        #keras_cv.layers.RandomFlip(mode=\"horizontal\", bounding_box_format=\"xyxy\"),\n",
    "        #keras_cv.layers.RandomRotation(factor=0.2, bounding_box_format=\"xyxy\"),\n",
    "        keras_cv.layers.JitteredResize(\n",
    "            target_size=(640, 640),\n",
    "            scale_factor=(1, 1),\n",
    "            bounding_box_format=\"xyxy\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d8b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_ds(train_data):\n",
    "    train_ds = train_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(BATCH_SIZE * 8)\n",
    "    train_ds = train_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    train_ds = train_ds.map(augmenter, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bee2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sub_train_data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b552c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_ds_all = create_train_ds(sub_train_data_all)\n",
    "\n",
    "sub_train_ds_1 = create_train_ds(sub_train_data_1)\n",
    "sub_train_ds_2 = create_train_ds(sub_train_data_2)\n",
    "sub_train_ds_3 = create_train_ds(sub_train_data_3)\n",
    "sub_train_ds_4 = create_train_ds(sub_train_data_4)\n",
    "\n",
    "#sub_trains_ds_zipped = tf.data.Dataset.zip(sub_train_ds_all, sub_train_ds_1, sub_train_ds_2, sub_train_ds_3, sub_train_ds_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c4d13d07a9021",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resizing = keras_cv.layers.JitteredResize(\n",
    "    target_size=(640, 640),\n",
    "    scale_factor=(1, 1),\n",
    "    bounding_box_format=\"xyxy\",\n",
    ")\n",
    "\n",
    "def create_val_ds(val_data):\n",
    "    val_ds = val_data.map(load_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.shuffle(BATCH_SIZE * 8)\n",
    "    val_ds = val_ds.ragged_batch(BATCH_SIZE, drop_remainder=True)\n",
    "    val_ds = val_ds.map(resizing, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dfa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_val_ds_all= create_train_ds(sub_val_data_all)\n",
    "\n",
    "sub_val_ds_1 = create_train_ds(sub_val_data_1)\n",
    "sub_val_ds_2 = create_train_ds(sub_val_data_2)\n",
    "sub_val_ds_3 = create_train_ds(sub_val_data_3)\n",
    "sub_val_ds_4 = create_train_ds(sub_val_data_4)\n",
    "\n",
    "#sub_val_ds_zipped = tf.data.Dataset.zip(sub_val_ds_all, sub_val_ds_1, sub_val_ds_2, sub_val_ds_3, sub_val_ds_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dfd9ee0435599",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_dataset(inputs, value_range, rows, cols, bounding_box_format):\n",
    "    inputs = next(iter(inputs.take(1)))\n",
    "    images, bounding_boxes = inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=value_range,\n",
    "        rows=rows,\n",
    "        cols=cols,\n",
    "        y_true=bounding_boxes,\n",
    "        scale=5,\n",
    "        font_scale=0.2,\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        class_mapping=class_mapping,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_dataset(sub_train_ds_all, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=1, cols=2)\n",
    "visualize_dataset(sub_val_ds_all, bounding_box_format=\"xyxy\", value_range=(0, 255), rows=1, cols=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ed721cc85379c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dict_to_tuple(inputs):\n",
    "    return inputs[\"images\"], inputs[\"bounding_boxes\"]\n",
    "\n",
    "def create_train_val_tuple(train_ds,val_ds):\n",
    "    train_ds = train_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d4805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_ds_all,sub_val_ds_all=create_train_val_tuple(sub_train_ds_all,sub_val_ds_all)\n",
    "sub_train_ds_1,sub_val_ds_1=create_train_val_tuple(sub_train_ds_1,sub_val_ds_1)\n",
    "sub_train_ds_2,sub_val_ds_2=create_train_val_tuple(sub_train_ds_2,sub_val_ds_2)\n",
    "sub_train_ds_3,sub_val_ds_3=create_train_val_tuple(sub_train_ds_3,sub_val_ds_3)\n",
    "sub_train_ds_4,sub_val_ds_4=create_train_val_tuple(sub_train_ds_4,sub_val_ds_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e170d82fe24bd7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aa6f584e4c14f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_backbone(backbone):\n",
    "    backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "        \"yolo_v8_xs_backbone_coco\",\n",
    "         load_weights=True \n",
    "    )\n",
    "    return backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c96ab8f16010f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df5fe5c59a00aa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_optimizer():\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        global_clipnorm=GLOBAL_CLIPNORM, #This ensures that gradients, which influence the model’s parameter updates, \n",
    "                                        # don’t become exceedingly large and destabilize training.\n",
    "    )\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd6e9a7ee37a17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define YOLO8Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e5f43561951061",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_model(num_classes):\n",
    "    model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=num_classes, #the number of object classes\n",
    "    bounding_box_format=\"xyxy\",\n",
    "    backbone=define_backbone(\"yolo_v8_xs_backbone_coco\"),\n",
    "    fpn_depth=1,\n",
    ")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75549b0bd6aa448",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71693c31705379",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    model.compile(\n",
    "    optimizer=define_optimizer(), \n",
    "    classification_loss=\"binary_crossentropy\", #calculates the discrepancy between anticipated class probabilities and actual class probabilities\n",
    "    box_loss=\"ciou\" # box_loss -> measure the difference between the predicted bounding boxes and the ground truth\n",
    "                    # he Complete IoU (CIoU) metric is used, which not only measures the overlap between predicted and ground truth bounding \n",
    "                    # boxes but also considers the difference in aspect ratio, center distance, and box size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd02b3a86101acca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc14bcd28e72b7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss',  \n",
    "                               patience=3,          \n",
    "                               restore_best_weights=True)  \n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "def fit_model(model, train_data, validation_data):\n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=EPOCH,\n",
    "        callbacks=early_stopping,\n",
    "        #callbacks=[EvaluateCOCOMetricsCallback(val_ds, \"workspace/models/yolo_coco_model.h5\")],\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d3178697d79ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define NonMaxSuppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d60b0d51b9293",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def define_NonMaxSuppression(model):\n",
    "    model.prediction_decoder = keras_cv.layers.NonMaxSuppression(\n",
    "        bounding_box_format=\"xyxy\",\n",
    "        from_logits=True,\n",
    "        iou_threshold=0.9,\n",
    "        confidence_threshold=0.5\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab3681e4f8e05b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define a base model, compile the base model and then loads the weights from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882e067f7329069",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_weight_model(model_path):\n",
    "    base_model = define_model(len(class_mapping))\n",
    "    compile_model(base_model)\n",
    "    loaded_model = base_model.load_weights(model_path)\n",
    "    return  loaded_model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d40400ff2d63b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save weights of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47feba0eebb6d985",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_weights(model, name):\n",
    "    save_model_path = MODEL_PATH + name\n",
    "    model.save_weights(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0c1e8d8d2b27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define visualization methode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2516f2d228dcce8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_detections(model, dataset, bounding_box_format, class_mapping):\n",
    "    images, y_true = next(iter(dataset.take(1)))\n",
    "    y_pred = model.predict(images)\n",
    "    y_pred = bounding_box.to_ragged(y_pred)\n",
    "    visualization.plot_bounding_box_gallery(\n",
    "        images,\n",
    "        value_range=(0, 255),\n",
    "        bounding_box_format=bounding_box_format,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        scale=5,\n",
    "        rows=1,\n",
    "        cols=1,\n",
    "        show=True,\n",
    "        font_scale=0.4,\n",
    "        class_mapping= class_mapping ,#class_mapping,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3796fc7867b4fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define and compile model for sub_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04f885e2c1b782",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_coco_sub = define_model(43)\n",
    "compile_model(yolo_coco_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218a7e8c569fabd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "define and compile model for main_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66bf3db6b01bc7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yolo_coco_main = define_model(len(main_class_mapping))\n",
    "compile_model(yolo_coco_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_MODEL_PATH = MODEL_PATH+'/main_bbox_detector_model.h5'\n",
    "\n",
    "#yolo_coco_main.load_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1769fff44ecc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit main_bbox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9851752000ef9c7a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main_bbox_history = fit_model(yolo_coco_main, main_train_ds, main_val_ds )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0665310e4b897",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fit sub_bbox_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d130532e0fbb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yolo_coco_sub.fit(sub_trains_ds_zipped, epochs=5, validation_data=sub_val_ds_zipped)\n",
    "\n",
    "sub_bbox_history_all= fit_model(yolo_coco_sub,sub_train_ds_all,sub_val_ds_all )\n",
    "\n",
    "NEWSUB_5_SETS_MODEL_PATH = MODEL_PATH+'/new_sub_bbox_detector_model_5_datasets.h5'\n",
    "#yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "#yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "#sub_bbox_history_1 = fit_model(yolo_coco_sub, sub_train_ds_1, sub_val_ds_1 )\n",
    "\"\"\"\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_2 = fit_model(yolo_coco_sub, sub_train_ds_2, sub_val_ds_2 )\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_3 = fit_model(yolo_coco_sub, sub_train_ds_3, sub_val_ds_3 )\n",
    "yolo_coco_sub.save_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "yolo_coco_sub.load_weights(NEWSUB_5_SETS_MODEL_PATH)\n",
    "sub_bbox_history_4 = fit_model(yolo_coco_sub, sub_train_ds_4, sub_val_ds_4 )\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4529b5fb",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d999d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Box Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['box_loss'], label='Training Box Loss')\n",
    "    plt.plot(history.history['val_box_loss'], label='Validation Box Loss')\n",
    "    plt.title('Training and Validation Box Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Box Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Klassen Loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['class_loss'], label='Training Class Loss')\n",
    "    plt.plot(history.history['val_class_loss'], label='Validation Class Loss')\n",
    "    plt.title('Training and Validation Class Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Class Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6406faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(sub_bbox_history_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb0a670477657e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualization main bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a74fd92642989",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#visualize_detections(yolo_coco_main, dataset=main_val_ds, bounding_box_format=\"xyxy\", class_mapping= main_class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc464b76f7c4b71",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualization sub bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d209caeefc4f547",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_detections(yolo_coco_sub, dataset=sub_val_ds_all, bounding_box_format=\"xyxy\", class_mapping= class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd044daa5edc01b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b9e1271bfded2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAIN_MODEL_PATH = MODEL_PATH+'/main_bbox_detector_model.h5'\n",
    "\n",
    "#yolo_coco_main.save_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ad9bcda410868",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "load model for sub_bbox and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa88f862ef2d6244",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#yolo_coco_sub.load_weights(MAIN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a8f31b3f9f3c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "visualize a prediction with loaded_sub_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb72e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_image(image_path, model):\n",
    "    image =  cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (640, 640))\n",
    "    resized_image = np.expand_dims(resized_image, axis=0)  \n",
    "\n",
    "    #predict\n",
    "    predictions = model.predict(resized_image)\n",
    "    boxes = predictions['boxes']\n",
    "    confidence = predictions['confidence']\n",
    "    classes = predictions['classes']\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for box, conf, cls in zip(boxes[0], confidence[0], classes[0]):\n",
    "        if conf > 0.1:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            #label = f\"Class {cls} ({conf:.2f})\"\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none')#, label=label)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaac925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_image(image_path, model):\n",
    "    image = cv2.imread(image_path)\n",
    "    resized_image = cv2.resize(image, (640, 640))\n",
    "    resized_image = np.expand_dims(resized_image, axis=0)  \n",
    "\n",
    "    # Predict\n",
    "    predictions = model.predict(resized_image)\n",
    "    boxes = predictions['boxes']\n",
    "    confidence = predictions['confidence']\n",
    "    classes = predictions['classes']\n",
    "\n",
    "    sorted_indices = np.argsort(-confidence[0])  \n",
    "    boxes = boxes[0][sorted_indices]\n",
    "    confidence = confidence[0][sorted_indices]\n",
    "    classes = classes[0][sorted_indices]\n",
    "\n",
    "    unique_classes = np.unique(classes)\n",
    "    selected_indices = []\n",
    "\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(classes == cls)[0]\n",
    "        if len(cls_indices) > 0:\n",
    "            best_idx = cls_indices[0]  \n",
    "            selected_indices.append(best_idx)\n",
    "\n",
    "    selected_boxes = boxes[selected_indices]\n",
    "    selected_confidence = confidence[selected_indices]\n",
    "    selected_classes = classes[selected_indices]\n",
    "\n",
    "    iou_threshold = 0.5\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for box, conf, cls in zip(selected_boxes, selected_confidence, selected_classes):\n",
    "        if conf > 0.1:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            label = f\"Class {cls} ({conf:.2f})\"\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor='r', facecolor='none' ,label=label)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#lade bild\n",
    "image_path1 = (IMAGE_PATH+ '/640x640_main_classes_scaled/0_4.jpg') \n",
    "image_path2 = (ORIGINAL_IMAGE_PATH+'/image_0004.jpg') \n",
    "predict_on_image(image_path1, yolo_coco_sub)\n",
    "predict_on_image(image_path2, yolo_coco_sub)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d43ee844c1840",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EvaluateCOCOMetricsCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, data, save_path):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.metrics = keras_cv.metrics.BoxCOCOMetrics(\n",
    "            bounding_box_format=\"xyxy\",\n",
    "            evaluate_freq=1e9,\n",
    "        )\n",
    "\n",
    "        self.save_path = save_path\n",
    "        self.best_map = -1.0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.metrics.reset_state()\n",
    "        for batch in self.data:\n",
    "            images, y_true = batch[0], batch[1]\n",
    "            y_pred = self.model.predict(images, verbose=0)\n",
    "            self.metrics.update_state(y_true, y_pred)\n",
    "\n",
    "        metrics = self.metrics.result(force=True)\n",
    "        logs.update(metrics)\n",
    "\n",
    "        current_map = metrics[\"MaP\"]\n",
    "        if current_map > self.best_map:\n",
    "            self.best_map = current_map\n",
    "            self.model.save(self.save_path)  # Save the model when mAP improves\n",
    "\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f47b37997236a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fake Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cd42883ee81fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:34:31.996115700Z",
     "start_time": "2024-01-09T13:34:31.924042600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alexej\\miniconda3\\envs\\simple-htr-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "# import keras_cv\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow import keras\n",
    "#from keras_cv import bounding_box\n",
    "#from keras_cv import visualization\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.models import Sequential, model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de25d596d2429a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:36:31.489332400Z",
     "start_time": "2024-01-09T13:36:31.390Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all XML file paths in path_annot and sort them\n",
    "xml_files = sorted(\n",
    "    [\n",
    "        os.path.join(\"data_zettel/Annotations\", file_name)\n",
    "        for file_name in os.listdir(\"data_zettel/Annotations\")\n",
    "        if file_name.endswith(\".xml\")\n",
    "    ]\n",
    ")\n",
    " \n",
    "# Get all JPEG image file paths in path_images and sort them\n",
    "jpg_files = sorted(\n",
    "    [\n",
    "        os.path.join(\"data_zettel/Annotations\", file_name)\n",
    "        for file_name in os.listdir(\"data_zettel/Annotations\")\n",
    "        if file_name.endswith(\".jpg\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fec2c06c5208d6f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T13:36:31.894495600Z",
     "start_time": "2024-01-09T13:36:31.854394800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_class_id(classes, cls):    \n",
    "    class_ids = [\n",
    "        list(class_mapping.keys())[list(class_mapping.values()).index(cls)]\n",
    "        for cls in classes\n",
    "    ]\n",
    "    return class_ids\n",
    "\n",
    "def create_box(bbox):\n",
    "    xmin = float(bbox.find(\"xmin\").text)\n",
    "    ymin = float(bbox.find(\"ymin\").text)\n",
    "    xmax = float(bbox.find(\"xmax\").text)\n",
    "    ymax = float(bbox.find(\"ymax\").text)\n",
    "    return [xmin, ymin, xmax, ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2298a2e644551a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:16:37.513681500Z",
     "start_time": "2024-01-09T14:16:37.459396500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_annotation_fake(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    image_name = root.find(\"filename\").text\n",
    "    image_path = f'data_zettel/filled_resized/{image_name}'\n",
    "\n",
    "    boxes = []\n",
    "    classes = []\n",
    "    main_classes = []\n",
    "    sub_classes = []\n",
    "    main_boxes = []\n",
    "    sub_boxes = []\n",
    "    values = []\n",
    "    \n",
    "    for obj in root.iter(\"object\"):\n",
    "        cls = obj.find(\"name\").text\n",
    "        classes.append(cls)\n",
    "        \n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        boxes.append(create_box(bbox))\n",
    "        \n",
    "        # main labels\n",
    "        if (cls == 'Wohnsitz_waehrend_Ausbildung') or (cls == 'Ausbildung') or (cls == 'Person') or (cls == 'Wohnsitz'):\n",
    "            main_classes.append(cls)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            main_boxes.append(create_box(bbox))\n",
    "        else:\n",
    "            attributes = obj.findall(\"attributes/attribute\")\n",
    "            for attribute in attributes:\n",
    "                value_element = attribute.find(\"value\")\n",
    "                if value_element is not None:\n",
    "                    value = value_element.text\n",
    "                    if value is None:\n",
    "                        value = \"\"\n",
    "                    elif value is None or value.lower() in [\"true\", \"false\"]:\n",
    "                        # Wenn der Wert None oder \"True\"/\"False\" ist, überspringe diese Iteration\n",
    "                        continue\n",
    "                    \n",
    "                    # Versuche, den Wert in einen Float zu konvertieren\n",
    "                    try:\n",
    "                        float_value = float(value)\n",
    "                        int_value = int(float_value)\n",
    "                        values.append(str(int_value))  # Hier wird der Integer in einen String umgewandelt\n",
    "                    except ValueError:\n",
    "                        # Wenn die Konvertierung fehlschlägt, füge den originalen Wert zur Liste hinzu\n",
    "                        values.append(value)\n",
    "                    break\n",
    "            \n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            sub_boxes.append(create_box(bbox))\n",
    "            sub_classes.append(cls)\n",
    "    return image_path, sub_boxes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4955cb22409e20f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:16:39.218790200Z",
     "start_time": "2024-01-09T14:16:39.140629200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 1067.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42', '21012001', '4', 'NR', '0', 'Puffer Straße', '21', 'NR', '49928', 'Arts', 'Lüpfer Weg', 'Master', '11112004', '21998', 'Span', '0', '0', '0', '0', '0', 'Worskpace Hochschule', '0', '0', 'NR', '217778210341982', 'Patron', 'Linda', 'tronan', 'Pindra']\n",
      "['Washin', '1', '0', '0', '0', '22219', '201', 'Freedom house', 'Ar', '5', '0', 'DE', '321', 'Berlin', '92187', 'Sandrin Universität', '1212001', 'Chimie', 'Bachelor', '0', '0', 'BE', '201022467349110', 'Zama', 'Zura', 'Bana', 'Weißerweg', '0', 'Lindor']\n",
      "['Babau', '234902399876543', 'BA', '0', '1', '0', 'Master', 'Biologie', 'Bilden Bibau', '0', '0', '0', 'Lize', 'Betz', 'Noran', 'Lotze', '21122003', '0', '2', '0', 'Antag Weg', '41', 'DE', '19227', 'Münich', '221', '78219', '0', 'Nierdan']\n",
      "['0', '1', '0', 'Bibaa Hochschule', 'Mathe', 'Master', '0', '0', 'Zo', '202193424982109', 'Luzan', 'Birzan', 'Luzan', 'Purshan', '11010192', '2', '5', 'NR', '0', 'Lianpina', '42', 'LP', 'Brobon', 'Bremerstraße', '49', 'Kl', '21983', 'Brebau', '0']\n",
      "['Stadau', 'Lor Weg', '0', '0', '0', 'Hagen', 'Busslin Weg', '21210', '241', '0', '1', 'Spand Bredau', 'Informatik', 'Master', '0', 'BR', '124908723410234', 'Liza', 'Bitz', 'Noron', 'Lotz', '11111999', '102101', 'DE', '0', '221', 'Br', 'DE', '22222']\n",
      "['21112101', '1', 'Bremen Hochschule', 'Median theory', 'Master', '0', '123456789120321', 'Namener', 'Bubaran', 'Lizadan', 'Bidan', '11212000', 'Kr', 'Br', '0', '0', 'Lützen Weg', '221', 'Hamburg', 'Hamburger Weg', '421', 'Kr', '21102', 'Hamburg', '0', '0', '1', '0', '0']\n",
      "['Master', '11211010', '3', 'NR', 'Br', '0', 'Bayerner Universität', 'Physik', 'Hamburg', '0', '0', 'Br', '20101', '210101111000111', 'Meiners', 'Liparan', 'Lupun', 'London', '21111092', 'Blauer Weg', '122', '0', 'Ai', '1', 'Spandau', '210', 'BR', 'Hauluwer Straße', '0']\n",
      "['0', 'Bayernerman', 'Bayern', '27720', 'Birland', 'Nonarin', 'Orinano', 'BN', 'Bacherlor', 'Andrea', 'Farbe Forschung', 'NB', '0', '0', '7210', 'Ki', '24', '222101263493100', '0', 'DR', '21028', 'Hand Straße', '0', 'Flavour Schule', '11022003', '21012009', '0', '0', '2']\n",
      "['14728', 'Dardon', '21982', '2B', '123', 'Straßendan Weg', 'Weideman', '21', 'Linderhof', '0', 'BR', '21111908', '11111111', 'pdardon', 'Runger', 'Linpada', 'Linpan', '123498760124578', 'Li', '0', 'Bacherlor', 'Wirtschaft', 'Pandau Hochschule', '0', '1', '0', '0', '0', '0']\n",
      "['0', '0', '1', 'Master', '1', 'Produkte', 'Mandau Universität', '0', '0', '0', '0', 'Bremen', '21010', '21', 'Gironau Straße', 'Baron', '21010', '210', 'Gröpelingen', '0', 'Br', 'BL', '11282112', '1112008', 'Bidazon', 'Minoros', 'Biaron', 'Adminor', 'Bo']\n",
      "['30403', '0', 'Oslo', '0', '358', '0', '3101985', '2', 'swedisch', '1', 'Obdachlosstr', '6', '358', 'Pokemon Trainer', '1', 'Master', '358', 'Ketchup', 'Arschin', 'Pikachu', '0', 'GER', '0', 'Karlsruhe Training Centre', '358', '0', '358', 'Karlsruhe', '0']\n",
      "['Bremen', '28208', 'GER', 'Im Keller', '420', '0', '0', '0', '0', '1', '0', '0', '1', 'Auberginestr', '0', 'russisch', '0', '0', '0', '7121995', 'Nottingham', '0', 'Doktor', 'Philosophie', '1', 'Freud', 'Sigmund', 'Universität Bremen', 'Bob']\n",
      "['Maria', '23208', 'Polen', 'DE', 'Kleinberg', '0', 'Toujo', '0', '1', '1', '100', 'Kleinbergerstraße', 'Deutscher', 'Polin', 'Bachelor', 'Informatik', 'Kleinberger Hochschule', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0']\n",
      "['0', '0', '0', '0', '3031998', '0', '0', '1', 'Deutscher', 'Bachelor', '0', '0', '0', 'Tanjo', '0', 'Maria', 'Polen', '0', '0', '1', 'Polin', '2', 'Kleinbergerstraße', '23208', 'Weinberger Hochschule', 'Informatik', '144', 'DE', 'Weinberg']\n",
      "['0', 'London', 'Sherlock', 'Master', '357', '358', 'Moriarty', 'Holmes', '3', '356', '0', '1', '0', '1', '357', '357', '0', '357', '49328', 'GER', '301', 'Longjohnstr', 'englisch', 'englisch', '11111987', 'London', 'Detective', 'London Universität', '357']\n",
      "['0', '0', 'Polin', 'Deutscher', 'Informatik', 'Tanjo', '0', 'Weinberg', '23208', 'DE', '100', '0', '0', '0', '1', '0', '0', '0', '0', 'Weinbergerstraße', 'Maria', '0', '0', '1', 'Weinberger Hochschule', 'Bachelor', 'Polen', '3031998', '2']\n",
      "['Peter', 'Hochschule Dresden', 'Master', 'Demonology', 'GER', '204', 'Dämonstr', 'Parker', '1', '0', '0', '0', '42066', '359', '1', 'deutsch', 'Dresden', '358', '0', '0', '1', '0', '0', '0', 'Spiderman', 'Dresden', '0', '31052002', '0']\n",
      "['DE', '0', '1', '1', '0', '100', '3031998', '0', '0', 'Polen', '0', '0', '0', '0', '0', '0', '0', 'Bachelor', 'Weinberg', '23208', 'Weinbergstrasse', 'Deutscher', 'Polin', '2', 'Maria', 'Taujo', '0', 'Informatik', 'Weinberger Hochschule']\n",
      "['BREMEN', 'ELIZABETH', 'UNIVERSITÄT BREMEN', 'JOJO', 'AMSTERDAM', '10011990', '28150', '355', 'deutsch', '355', '354', '1', '356', '2', '356', '355', '355', 'STARK', '355', '355', '355', '356', 'deutsch', 'BACHELOR', '1', 'NEUSTADTERSTR', '50', 'GER', 'BIOLOGIE']\n",
      "['Polen', '0', '3031998', '0', '0', '0', '0', '2', '0', 'Maria', '0', 'Weinberg', '1', 'Informatik', 'Bachelor', '0', '0', 'Toujo', '1', '0', '0', '0', 'DE', '100', 'Weinbergstrasse', 'Deutscher', '23208', 'Weinberger Hochschule', 'Polin']\n",
      "['2', 'Naruto', 'Uzumaki', 'Konoha Academy', 'Ninjutsu', 'Master', '3', 'Konoha', '89320', 'GER', 'Ninjastr', 'japanisch', 'japanisch', '2', '0', '1', '3', '3', '25031999', '2', 'Kyuubi', '0', '4', '0', '3', 'Konoha', '2', '1', '2']\n",
      "['Polen', '1', '3031998', '2', 'DE', '23208', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', 'Weinberger Hochschule', 'Informatik', 'Weinber', '100', 'Weinbergerstraße', 'Deutscher', 'Polin', 'Maria', 'Toujo', '0', 'Bachelor']\n",
      "['Bremen', 'Hochschule Bremen', 'Mark', 'Robert', 'Potsdam', '5092001', '359', '0', '0', 'deutsch', '0', '1', '358', '0', 'Henry', '0', '358', '0', '1', 'Medien Informatik', '0', '28320', 'GER', '150', 'Parkstraße', '1', '0', '0', 'Bachelor']\n",
      "['0', '0', '0', 'Polen', '3031998', '0', '1', 'Toujo', 'maria', '2', 'Weinbergstraße', '1', '0', '0', '0', '0', '0', '0', '0', 'Deutscher', '100', 'DE', '23208', 'Weinberg', 'Weinberger Hochschule', 'Informatik', 'Bachelor', '1', 'Polin']\n",
      "['0', '0', 'Bachelor', 'Fly Catcher', 'Hochschule Ribbit', 'Ponderberg', '48957', 'GER', '39', 'FROG', '0', '0', '5061996', 'Amazon', 'The', 'Kermit', '0', '0', '0', '0', '1', '0', '0', '0', '0', '1', 'polisch', '1', 'Pondstraße']\n",
      "['1', '0', '0', '0', '0', '0', '0', '0', '0', 'Polin', '2', '3031998', '1', '0', 'Polen', 'Maria', 'Toujo', '0', 'Deutscher', 'Weinbergerstraße', '100', 'DE', '0', 'Informatik', 'Weinberger Hochschule', 'Bachelor', '0', '23208', 'Weinberg']\n",
      "['Bros', 'Osnabrück', '13452', 'GER', '12', 'Mario', '0', 'Master', '0', '0', 'itanlianisch', '4', '0', '0', '0', '1', 'Japan', 'Super', '0', '0', '0', 'Trampoline Universität', 'Springen', '1081987', '0', '1', 'Pipestr', '0', '0']\n",
      "['0', '0', '0', '3031998', '1', '0', '2', '0', 'Polin', '0', '23208', '0', '0', '1', 'Weinberg', '0', '0', '0', 'Polen', 'DE', 'Bachelor', 'Informatik', 'kleinberger Hochschule', 'Maria', 'Deutscher', '100', 'WeinbergerStr.', '0', 'Toujo']\n",
      "['120', '23032000', 'Denver', 'kcao', 'Bachelor', 'Racing', 'Universität Berlin', '1', '357', '1', '0', '0', '0', '357', '357', '357', '0', '356', 'Berlin', '53250', 'GER', 'Ferraristr', 'amerikanisch', '3', '357', 'Lightning', 'McKing', '357', '357']\n",
      "['0', 'Weinberger Hochschule', 'Informatik', 'Bachelor', 'Toujo', 'Weinbergerstrasse', '100', '0', '0', '0', '0', '0', '1', '0', '23208', 'DE', '0', '1', '0', '0', '0', '0', '0', 'Maria', 'Polen', '3031998', '2', 'Polin', 'Deutscher']\n",
      "['Bob', '13778', 'Bachelor', 'Informatik', 'Leuchtenburg', '28779', 'DE', 'Tj', 'Sponge', '0', 'Fachhochschule Bielefeld', '0', '0', '0', '0', '0', '88', 'Bikini Bottom', '0', '0', 'Deutsch', '0', '0', '1', '0', '1', '8072000', 'Bremen', '0']\n",
      "['0', '0', '1', '0', '0', '0', '0', '1', '1', '0', '0', '0', '0', '0', '0', '1', '0', 'Fritz', 'Deutsche', 'Auf der Koppel', '23', 'Steuern, Digitale Wirtschaft', 'Oldenburg', '26121', '3031999', 'Oldenburg', 'Marina', '10.05.2025', 'Jade Hochschule Oldenburg']\n",
      "['1041970', 'Sponge', 'Patrick', 'Star', '787', '0', 'Master', 'Chemie', 'Touro College Berlin', 'USA', '77777', 'Bikini Bottom', '778', 'Bikini', '0', 'Amerikanisch', '4', 'Bottom', '0', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0']\n",
      "['0', 'Deutsch', 'Kölner', 'Bachelor', '28888', 'Köln', '0', '0', '1', '1', 'DE', '0', '0', '0', '0', '0', '0', '0', '3031977', '1', '0', '0', 'Köln', 'Freie Universität Berlin', 'Informatik', 'Igor', 'Martin', '778', 'Kölner']\n",
      "['Bayern', 'Biologie', '0', 'Platz', '1', 'DE', '28998', 'Sascha', 'Jens', '1', '1', '0', '0', '1061999', '0', '0', '0', '0', '0', '0', '1', '0', 'Moskau', 'Asimov', 'Hertie School', 'Bachelor', '0', '0', '0']\n",
      "['0', 'DE', '28970', 'Berlin', '0', '0', '1', '0', '0', '0', '0', '1', '0', '0', '0', '0', 'Steinbeis Hochschule', 'Sport', 'Master', '0', 'Lev', 'Markus', 'Leon', '0', '1', 'Bayern', '2081997', 'Wakanda', '999']\n",
      "['Quadriga Hochschule Berlin', 'Bachelor', '1', 'Elektrotechnik', '0', '0', '7778765', 'Markus', '0', 'Berlin', '0', '0', '0', '0', '0', 'Malte', 'Gustaf', 'Bemen', 'Guten', '120', '0', '0', '0', '0', 'DE', '28779', '0', '1651960', '3']\n",
      "['3.08.2023', 'Hotelfachmann', 'Schulzentrum 28219 Bremen', 'Weng', '13408967853210', '0', 'Schwering', 'Emily', '0', '0', '0', 'Schwanewede', '28790', '9', 'Puschkinstraße', 'Deutsch', '24121999', '0', '0', '0', '1', '0', '0', '0', '1', '0', '0', '0', '1']\n",
      "['1', '0', '0', '0', '0', '10051996', '0', 'Deutsche', 'Hakenwehrstraße', '7', '28779', 'Bremen', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '1', 'Berlin', 'Marcus', 'Kraft', '0', 'Architektur', 'Jade Hochschule Wihland']\n",
      "['Beckedorfstraße', 'Elektrotechnik und Informatik', 'Kevin', 'Bremen', '22031998', 'Deutsche', '17', '28790', 'Beckedorf', '0', '0', '0', '0', '1', '1', '0', '0', '0', '1', 'Hochschule Bremen', 'Krause', '0', '0', '1', '0', '0', '10.09.2024', '0', '0']\n",
      "['0', '14051984', '0', '0', 'Amy', 'Amy', '1', 'USA', '0', '1', '0', '0', 'Potsdam', 'Koch', 'Zuckerberg', '0', '0', '1', 'Mark', '0', '0', '0', 'New York', '0', '0', '1', 'Kalifornien', 'Hacker Way', '94025']\n",
      "['Funker', '0', '0', 'Kassel', '121314752741397', '7', 'Kassel', '0', '0', '0', '0', '0', '0', '1', '0', 'Nebraska', 'Senator str', '0', '0', 'USA', '0', 'Warren', 'Buffet', 'Ohama', '0', 'Amy', '1', '30081930', '50735']\n",
      "['94043', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', 'California', 'USA', '2', 'Berlin', 'Model', 'Brin', 'Sergej', '21081973', '5', 'Ruski', '0', 'Amphitheatre Parkway']\n",
      "['0', '2', '28101995', 'Seatle', 'Bill', 'Gates', '123456789001001', 'Stuttgart', 'Amy', '0', 'Amy', '0', '0', 'Kellner', '0', '0', 'Redmond Woods', '0', 'Stuttgart', '0', '1', '0', '148th Ave NE', '98052', 'USA', '0', '0', '14', '0']\n",
      "['0', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', 'Manager', 'Amy', 'Amy', '0', '148th Ave NE', '14', 'USA', '98052', 'Redmond Woods', '2', '24031956', 'Detroid', 'Steve', 'Ballmer', '0', 'Köln']\n",
      "['Michigan', '0', '0', '1', 'Wismar', '0', 'Page', 'Larry', '0', '26031973', 'Amy', 'Amy', '0', 'Amphitheatre Parkway', '7', 'Junker', '0', '1', '0', '0', '0', '0', '0', '1', 'USA', '24043', 'California', '0', '0']\n",
      "['28271', '0', '5', 'Austin', '0', '0', '0', '0', '0', '0', '1', '0', '17071944', 'Larry', 'Cloud Way', '1', '2300', '0', '0', '0', 'USA', '0', 'New York', '0', '0', 'Berlin', 'Ellison', '0', 'Gas, Wasser,...']\n",
      "['Schwabingen', 'Karlsruhe', 'informatiker', '0', '80538', '0', '0', 'New Mexiko', '2', '12011964', 'Albuqerque', 'Jeff', 'Bezos', '0', '0', '0', '0', 'Amy', '0', '0', '0', 'Parkstraße', '0', '1', '1', '37', '7', 'DE', '0']\n",
      "['0', 'Raubix', '0', '0', '0', 'Automatisierer', 'Rernard', '0', '5031949', 'Jeff', '0', 'München', '0', '0', '0', '0', '0', '0', '1', '1', 'Köln', '1', 'Arnault', '8', 'München', 'Ger', 'Franzose', 'Leopold str', '80802']\n",
      "['Berlin', 'Düsseldorf', '0', 'König', '0', '0', 'Musk', 'Elon', 'Pretoria', '28061971', '2', 'Afrika', '0', 'Ludwig-Prundte', '27', 'Ger', '12526', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0']\n",
      "['1', '0', '0', '0', '0', '0', 'Musterstadt', '0', '1', '0', '1', '0', '0', '0', 'Fakeort', 'Bachelor', 'Technische Universität', 'Maschinenbau', '0', '12345678910', 'Mülle', 'Max', '1011995', 'Deutsch', 'Deutsch', '0', 'Hauptstraße', '42', 'Deutschland']\n",
      "['0', 'Doktor', 'Deutsch', '0', '0', '1', '0', '0', '0', 'Laura', 'Fischer', '0', '789123', 'Behörde', 'BetriebswirtschaftslehrBetriebswirtschaftslehre', 'IW Globalstadt', '0', '0', '0', '1', '0', '0', '1', '0', '0', '10101990', 'Handelsburg', 'Becker', '0']\n",
      "['0', '0', 'Deutsch', '1041990', 'Phantasia', 'Clara', 'Schmidt', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '456874678910112', '0', '0', '1', '0', '0', 'Fotografie', 'Art Akademie Hamburg', 'Master']\n",
      "['Jonas', 'Fantasiewelt', '15051995', 'Deutsch', 'Abenteuerweg', 'DE', '33', '67890', 'Fantasiewelt', 'Maschinenbau', 'Bachelor', '0', '0', 'Weber', '0', '1', '0', 'Hochschule Berlin', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1']\n",
      "['Fischer', '0', '0', '0', '0', '0', 'Studio 2', '10', 'Kreativallee', '45678', 'HE', '5', 'Blumenweg', 'Deutsch', '2031992', 'Musterstadt', 'Lena', 'Bauer', '12345', 'Farbenstadt büro', '0', '0', '1', 'Bachelor', '1', 'Universität Farbenstadt', 'Farbenstadt', '98765', 'Grafikdesign']\n",
      "['David', 'Institut Forschungsstadt', 'Umweltwissenschaften', '0', 'Behörde', '65432', 'Neumann', 'Klein', 'Grüntal', '0', '20', 'Doktor', '8081988', 'Deutsch', '0', 'Waldpfad', '0', '0', 'Forscherweg', '0', '12', 'ID', '12321', 'Grüntal', '1', 'Forschungsstadt', '32123', '0', '1']\n",
      "['0', '0', '1', '0', '0', '1', '0', '0', 'Hochschule Bau', 'Architektur', 'Diplom', '0', 'Amt', '87654', 'Zimmermann', 'Sarah', 'Weber', 'Himmelsdorf', '16071994', 'Deutsch', 'Blütenstraße', '22', 'VI', '55678', 'Himmelsdorf', '87655', 'Konstruktionsstadt', '5', 'Planerallee']\n",
      "['Richter', 'Behörde', '32198', 'Felix', 'Lehmann', 'Bergdorf', '9091989', 'Deutsch', 'Österreichisch', '0', 'Med Fakultät', 'Allgemeinmedizin', 'Doktor', '0', '0', '1', '0', '0', '0', '0', '0', '0', '1', '0', '1', '0', '0', '0', '0']\n",
      "['0', '1', '0', '0', '0', 'Schmidt', 'Buchhausen', '14021991', '0', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', 'Wagner', 'Emily', '56789', 'Literaturwissenschaft', 'Uni Kulturstadt', 'Deutsch', 'Behörde', 'Doktor', '0']\n",
      "['Master of Science', 'TU Berlin', 'Deutsch', '21061993', 'Oliver', 'Digitalien', 'Neumann', 'Informatik', '0', '0', '654321', '0', '0', '0', '0', 'Schwarz', '0', '1', '0', '0', '1', '0', '0', '0', '0', '0', '1', '0', 'Amt']\n",
      "['0', '1', '0', '0', '1', 'Bremen', '28219', '0', '0', '282', 'Stader Str.', 'Deutsch', '5082004', 'Delmenhorst', 'Peter', 'Peterson', '0', 'Kaufmann', 'Biemen', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0']\n",
      "['1', '0', '1', '0', '0', '0', '1', '0', '0', '0', '0', '0', '0', '0', 'GER', 'Hohentor Universität', 'Informatik', 'Master', '0', 'Max', 'Max', 'Max', 'Maxstadt', '1012000', 'DE', 'Maxstraße', '5', '57312', 'München']\n",
      "['Harvard', 'deutsch', 'Main Street', '7', 'USA', '31057', 'Boston', '356', '356', 'Boston', '31061983', 'Computer', 'Bachelor', '357', '355', '358', '356', '1', '356', '356', '1', '358', '356', '358', '0', '356', '1', 'Zuckerberg', 'Mark']\n",
      "['555333678955000', 'Bremeramt', 'Deutsch', 'Master of language', '0', '0', '0', '0', 'Hannover', '0', '0', '0', '53210', 'POL', '1032004', 'Bremen', 'Meyer', 'Müller', 'Denis', '0', '0', '12', 'Karl-Marx-Weg', 'TH Hannover', '1052018', '2', 'deutsch', 'polnisch', '0']\n",
      "['25319', 'Wizard Master', '0', 'Harry', '0', 'Der Weg', '33', 'NL', 'Amsterdam', 'kroatisch', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', 'Wizard School Hogwarts', 'Magic Potions', 'Potter', 'Lars', 'Berlin', '12122003', '0']\n",
      "['0', '0', '0', 'Wolfgang', 'Johann', 'Goethe', '2', '1', '0', '0', 'Hochschule Hamburg', 'Mathe', 'Bachelor', '0', 'Berlin', '2', '0', '1', '1', '0', '1', '0', '29531', '10101955', 'deutsch', 'Hamburger Weg', '5', 'GER', 'Hamburg']\n",
      "['Stege', '356', '356', '357', '357', '357', '0', 'Universität Leipzig', 'Chemie', 'Master', '358', 'Max', 'Leipzig', '1012001', 'polnisch', 'Leipzig Straße', '3', 'GER', '29317', 'Leipzig', '1', '0', '0', '359', '1', '0', '1', '358', '356']\n",
      "['Chicago', 'Master of Science', '355', '355', '357', 'Computer Engineering', 'State University Chicago', 'deutsch', 'Street 2nd', '356', '57310', 'Bremen', '1', 'Bo', '0', 'Tim', 'USA', '5', '357', '1', '12122003', '0', '1', '356', '354', 'Bo', '0', '355', '355']\n",
      "['0', '0', '1', '0', '0', '0', '0', 'Monobachelor', 'Lehramt', 'Universität Heidelberg', '0', '0', '359', '1', '1', '0', '0', 'Heidelberg', '0', 'Blumen', 'Julia', 'Blumen', '3032005', 'deutsch', 'Friedrich Straße', '3', 'GER', '10135', 'Heidelberg']\n",
      "['Freiburg', 'Hochschule Albert', 'Soziale Arbeit', 'Bachelor', '0', 'Smith', 'Luisa', 'Smith', 'New York', '29052000', 'deutsch', 'Friedensweg', '5', 'GER', '79085', '1', '0', '0', '0', '0', '0', '1', '358', '0', '0', '0', '1', '0', '0']\n",
      "['0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '0', '0', '1', '0', '0', 'GER', 'FH Berlin', 'Informatik', 'Bachelor', 'Mark', 'Max', 'Berlin', 'Berlin', '30111999', 'deutsch', '0', 'Karl Straße', '5', '51350']\n",
      "['Bremen', '0', 'deutsch', '2', '14022001', 'Joachim', 'Jochen', '123456789101112', 'kleines Amt', '0', 'Bachelor of Science', 'Informatik', 'Hochschule Bremen', '12', 'DE', '28757', 'Bremen', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '0', 'Kuhdamm']\n",
      "['0', '1', '0', 'Universität Lübeck', 'Data Science', 'Bachelor of Science', '0', '0', '0', '0', '0', '0', '1', '0', '0', 'Kiel', 'Birne', 'Svenja', 'Apfel', '123456789012345', '5061999', 'deutsch', 'Hamurg', 'Schnelle Staße', '67', 'a', 'DE', '12', 'Verden']\n",
      "['12345', '1', '0', '0', '0', '0', '0', '0', '0', '0', 'Universität Bremen', 'Medieninformatik', 'Bachelor of Arts', '0', 'Beil', 'Helga', 'Axt', 'Lübeck', '2021980', 'deutsch', 'Baumallee', '1', 'DE', '1', 'Bremer Str.', '1', '28203', 'Bremen', '1']\n",
      "['0', 'Hochschule Hannover', 'Technische Informatik', 'Bachelor of Science', '0', '0', 'Dezernat 1', '1123411925300', 'Thorsten', 'Thomas', 'Lübeck', '5121980', '5', 'französisch', 'italientisch', '0', 'Lange Str.', '50', 'a', 'DE', '12333', 'Wels', '3560', '52', 'Bremen', '1', '0', '0', 'Kurze Str.']\n",
      "['Hochschule Hamburg', 'Elektrotechnik', 'Bachelor of Engineering', 'Haus', 'Anne', 'München', '30101999', 'deutsch', 'Mittelstraße', '1', 'b', 'DE', 'Verden', '22305', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', '1', '0', '0']\n",
      "['0', '0', '1', '0', '0', '0', '0', '0', '0', '0', '1', '1', '0', 'Bachelor of Engineering', 'Halle', '33650', 'DE', '5', 'Spitze Staße', 'polnisch', '11112011', 'Kiel', 'Zack', 'Elke', 'Zarrath', '0', '0', 'Mathematik', 'Universität Hamburg']\n",
      "['Bermerhaven', '0', '12032005', 'z', '5', 'deutsch', 'DE', '51234', 'Bremen', '0', '0', '0', '0', 'RTW Aachen', 'Maschinenbau', 'Bachelor of Engineering', '0', 'Tisch', 'Jasmin', 'Stuhl', 'Enge Staße', '54', '1', '1', '0', '0', '0', '0', '0']\n",
      "['Berlin', '77', 'Hohe Staße', '0', '1', '0', '0', 'Jens', 'Teller', '1', '0', '0', '198760234102365', 'Amt Sachsen', '0', '0', '0', '0', 'Bachelor of Arts', '0', 'Universität Berlin', 'BetriebswirtschaftslehrBetriebswirtschaftslehre', 'Berlin', '521917', 'deutsch', 'deutsch', 'b', 'DE', '15209']\n",
      "['Bremen', '0', 'Knopf', 'Hannes', '111991', '16705', '0', '0', '1', 'DE', '1', 'Flache Staße', 'deutsch', 'Master of Science', '0', '0', 'Mechatronik', 'Hochschule Bremerhaven', '0', '1', '0', '0', '0', '0', '0', '1', '0', '0', '0']\n",
      "['1', '1', '0', '0', 'Karsten', 'Kupfer', 'Hamburg', '0', '0', '10101990', '0', '0', 'deutsch', '0', '0', '0', '0', '0', '1', '55212', 'Hamburg', 'Universität Kiel', 'Tiefe Staße', '3001', 'DE', 'Stahl', '0', 'Master of Science', 'Energietechnik']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "ImageInfo = namedtuple('ImageInfo', ['path', 'boxes', 'values'])\n",
    "\n",
    "image_list = []\n",
    "for xml_file in tqdm(xml_files): \n",
    "    image_path, sub_boxes, values= parse_annotation_fake(xml_file)\n",
    "    image = ImageInfo(path=image_path, boxes=sub_boxes, values=values)\n",
    "    image_list.append(image)\n",
    "    print(values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73327eb722f2ea",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Crop ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8995ce981686184e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:16:41.482843900Z",
     "start_time": "2024-01-09T14:16:41.468218100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(image_list[5].boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dbc953849a6b899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-09T14:17:04.605779400Z",
     "start_time": "2024-01-09T14:16:42.004155500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "21012001\n",
      "4\n",
      "NR\n",
      "Puffer Straße\n",
      "21\n",
      "NR\n",
      "49928\n",
      "Arts\n",
      "Lüpfer Weg\n",
      "Master\n",
      "11112004\n",
      "21998\n",
      "Span\n",
      "0\n",
      "Worskpace Hochschule\n",
      "NR\n",
      "217778210341982\n",
      "Patron\n",
      "Linda\n",
      "tronan\n",
      "Pindra\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path_crops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, imgCropped)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path_crops\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     23\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(image\u001b[38;5;241m.\u001b[39mvalues[i])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Crop ROI\n",
    "save_path_crops = \"data_zettel/cropped_images\"\n",
    "\n",
    "import cv2\n",
    "def crop(xmin, ymin, xmax, ymax, image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    xmin = int(round(xmin))\n",
    "    ymin = int(round(ymin))\n",
    "    xmax = int(round(xmax))\n",
    "    ymax = int(round(ymax))\n",
    "    imgCropped = image[ymin:ymax, xmin:xmax]\n",
    "    return imgCropped\n",
    "\n",
    "\n",
    "for index, image in enumerate(image_list):\n",
    "    boxes = image.boxes\n",
    "    for i, box in enumerate(boxes):\n",
    "        xmin, ymin, xmax, ymax = np.array(box)\n",
    "        imgCropped = crop(xmin, ymin, xmax, ymax, image.path)\n",
    "        cv2.imwrite(f\"{save_path_crops}/{index}_{i}.jpg\", imgCropped)\n",
    "        with open(f\"{save_path_crops}/{index}_{i}.txt\", 'w') as file:\n",
    "            print(image.values[i])\n",
    "            file.write(image.values[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc51121e5bd01e2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617741a46be8f4e3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unsere Klassen\n",
    "#import handwriting.load_data as load_data\n",
    "import handwriting.load_transfer_data as load_transfer_data\n",
    "import handwriting.testing_models as testing_models # Use: build_model9v3(img_width, img_height, char) \n",
    "import utils.configs as cfg\n",
    "#Imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import random\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912de093664402a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cafc2f961d6b8d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config_path = \"utils\\configs.json\"\n",
    "\n",
    "config = cfg.Config(config_path)\n",
    "\n",
    "# Pipeline\n",
    "TRANSFER = bool(config.get_pipeline_parameter()[\"transfer\"])\n",
    "\n",
    "# Model Parameter\n",
    "MODEL_SAVE = bool(config.get_model_parameter()[\"save\"])\n",
    "MODEL_NAME = config.get_model_parameter()[\"name\"]\n",
    "IMAGE_WIDTH = config.get_model_parameter()[\"width\"] # default: 1024\n",
    "IMAGE_HEIGHT = config.get_model_parameter()[\"height\"] # default: 128\n",
    "\n",
    "# Directory Parameter\n",
    "MODEL_DIR_NAME = pathlib.Path(os.getcwd()).joinpath(config.get_directory_parameter()[\"model_dir\"])\n",
    "TEST_RESULT_DIR_NAME = pathlib.Path(os.getcwd()).joinpath(config.get_directory_parameter()[\"test_dir\"])\n",
    "DATA_BASE_PATH = config.get_directory_parameter()[\"data_base_path\"]\n",
    "\n",
    "# Training Parameter\n",
    "SAVE_HISTORY = bool(config.get_training_parameter()[\"save_history\"])\n",
    "EPOCHS = config.get_training_parameter()[\"epochs\"]\n",
    "BATCH_SIZE = config.get_training_parameter()[\"batch_size\"] # default: 32 - 48\n",
    "TF_SEED = config.get_training_parameter()[\"tf_seed\"] # default: 42\n",
    "LEARNING_RATE = config.get_training_parameter()[\"learning_rate\"]\n",
    "PATIENCE = config.get_training_parameter()[\"patience\"] # default: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_SAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ebf19d620853",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be0da09e6b20c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default: seed = 42\n",
    "# np.random.seed(TF_SEED)\n",
    "# tf.random.set_seed(TF_SEED)\n",
    "\n",
    "# Random\n",
    "#seed = random.randint(1, 1000)\n",
    "#np.random.seed(seed)\n",
    "#tf.random.set_seed(seed)\n",
    "print(LEARNING_RATE)\n",
    "print(TF_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e96e3bacfefa84",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load_data.print_samples(DATA_BASE_PATH)\n",
    "if False: # IAM Dataset\n",
    "    x_train_img_paths, y_train_labels = load_data.get_train_data()\n",
    "    x_val_img_paths, y_val_labels = load_data.get_validation_data()\n",
    "    #x_test_img_paths, y_test_labels = load_data.get_test_data()\n",
    "else:\n",
    "    x_train_img_paths, y_train_labels = load_transfer_data.get_train_data()\n",
    "    x_val_img_paths, y_val_labels = load_transfer_data.get_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc94188289acd0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"Training path: {x_train_img_paths[0:2]}\", y_train_labels[0:2])\n",
    "print(f\"Validation path: {x_val_img_paths[0:2]}\", y_val_labels[0:2])\n",
    "#print(f\"Testing path: {x_test_img_paths[0:2]}\", y_test_labels[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d396ce20431c83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c72e5da9634ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Has to be here because load data functions need to be called before\n",
    "import handwriting.tokenizer as tokenizer\n",
    "import handwriting.custom_image_generator as cgi\n",
    "\n",
    "# takes eternity\n",
    "#x_train, y_train = tokenizer.prepare_data(x_train_img_paths, y_train_labels) \n",
    "#x_test, y_test = tokenizer.prepare_data(x_test_img_paths, y_test_labels)\n",
    "\n",
    "#train_generator = cgi.CustomImageGenerator(x_train_img_paths, y_train_labels, BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(x_train_img_paths, y_train_labels, (IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "val_ds = tokenizer.prepare_dataset(x_val_img_paths, y_val_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "#test_ds = tokenizer.prepare_dataset(x_test_img_paths, y_test_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "#aug_train_ds = tokenizer.prepare_augmented_dataset(x_train_img_paths, y_train_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267aa06119792c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Show Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1843d431baf541",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "    ax = plt.subplots(4, 4, figsize=(32, 8))[1]\n",
    "\n",
    "    for i in range(16):\n",
    "        img = images[i]\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        img = tf.transpose(img, perm=[1, 0, 2])\n",
    "        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "        img = img[:, :, 0]\n",
    "\n",
    "        # Gather indices where label!= padding_token.\n",
    "        label = labels[i]\n",
    "        indices = tf.gather(label, tf.where(tf.math.not_equal(label, tokenizer.padding_token)))\n",
    "        # Convert to string.\n",
    "        label = tf.strings.reduce_join(tokenizer.num_to_char(indices))\n",
    "        label = label.numpy().decode(\"utf-8\")\n",
    "\n",
    "        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "        ax[i // 4, i % 4].set_title(label)\n",
    "        ax[i // 4, i % 4].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bfaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(load_transfer_data.max_len)\n",
    "print(load_transfer_data.characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a37a927a1a58b2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0b142ed5b54cf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To see the augmentations from CustomImageGenerator\n",
    "train_generator = cgi.CustomImageGenerator(x_train_img_paths, y_train_labels, tokenizer.batch_size, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "example_batch = train_generator[0]\n",
    "augmented_images = example_batch[0]['image']\n",
    "\n",
    "num_to_plot = 4\n",
    "fig, axes = plt.subplots(1, num_to_plot, figsize=(10, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(np.squeeze(augmented_images[i]), cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322476a450778bc6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomBrightness(0.5,value_range=(0, 1), seed=TF_SEED),\n",
    "        tf.keras.layers.RandomContrast(0.5,seed=TF_SEED)\n",
    "    ]\n",
    ")\n",
    "\n",
    "for data in train_ds.take(1):\n",
    "    images, labels = data[\"image\"], data[\"label\"]\n",
    "\n",
    "# Display the original image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(images[0].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "# Apply data augmentation to the image\n",
    "augmented_images = data_augmentation(images, training=True)\n",
    "\n",
    "# Display the augmented images\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 4, i + 2)\n",
    "    plt.imshow(augmented_images[i].numpy(), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(f\"Augmented Image {i+1}\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de342f6f643d2f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c9f0a326f83ce",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_keras_string =\"_weights.keras\"\n",
    "\n",
    "def model_load_weights_if_exists(model):\n",
    "    MODEL_MODEL_PATH = MODEL_NAME\n",
    "    MODEL_WEIGHT_PATH = MODEL_NAME + weights_keras_string\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, MODEL_MODEL_PATH)\n",
    "    model_weight_path = os.path.join(model_path, MODEL_WEIGHT_PATH)\n",
    "    print(model_path)\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Resuming Training where we left off!\")\n",
    "        model.load_weights(model_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678df9fcdf15f347",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    #model_load_weights_if_exists(model)\n",
    "        \n",
    "    prediction_model = keras.models.Model(model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output)\n",
    "    # checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, restore_best_weights=True)\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE * 0.2, min_lr=1e-6, verbose=1)\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[reduce_lr, early_stopping])    \n",
    "    return prediction_model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72291f",
   "metadata": {},
   "source": [
    "# Transfer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def load_model_and_weights():\n",
    "    weights_keras_string = \"_weights.keras\"\n",
    "    MODEL_MODEL_PATH = MODEL_NAME\n",
    "    MODEL_WEIGHT_PATH = MODEL_NAME + weights_keras_string\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, MODEL_MODEL_PATH)\n",
    "    model_weight_path = os.path.join(model_path, MODEL_WEIGHT_PATH)\n",
    "    model_weight_path = \"./handwriting/models/model9v3_xl/model9v3_xl_weights.keras\"\n",
    "    model_path = \"./handwriting/models/model9v3_xl\"\n",
    "    print(model_path)\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(\"Loading pre-trained model and weights...\")\n",
    "        model = load_model(model_path)\n",
    "        model.load_weights(model_weight_path)\n",
    "        print(\"Model and weights loaded successfully.\")\n",
    "        return model\n",
    "    else:\n",
    "        print(\"No pre-trained model or weights found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02884dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model_and_weights()\n",
    "char = len(tokenizer.char_to_num.get_vocabulary())\n",
    "model = testing_models.load_and_finetune_model(model, IMAGE_WIDTH, IMAGE_HEIGHT, char, LEARNING_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "prediction_model, history = train_model(model)\n",
    "\n",
    "total_duration = time.time() - start_time\n",
    "print(\"Gesamte Trainingsdauer: {time}s\".format(time=round(total_duration)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc27d01fe397042",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plot helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443bfc89b1a8fea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_history(history, name, dir_path, save_fig):\n",
    "    \"\"\"\n",
    "    Plottet die Historie des Trainings eines Models und speichert die in einem Verzeichnis ab \n",
    "\n",
    "    :param history: Das trainierte Modell\n",
    "    :param name: Name, wie das Modell gespeicht werden soll\n",
    "    :param name: Verzeichniss, wo der Plot gespeichert weren soll\n",
    "    :return: void\n",
    "    \"\"\"\n",
    "    metrics = history.history\n",
    "    _, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot für Trainings- und Validierungsverluste\n",
    "    ax1.plot(metrics['loss'], label='Training Loss', color='blue')\n",
    "    ax1.plot(metrics['val_loss'], label='Validation Loss', color='red')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss', color='black')\n",
    "    ax1.tick_params('y', colors='black')\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(0.0, 0.95))  \n",
    "\n",
    "    # Zweite Y-Achse für die Lernrate\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(metrics['lr'], label='Learning Rate', color='green')\n",
    "    ax2.set_ylabel('Learning Rate', color='black')\n",
    "    \n",
    "    ax2.set_yscale('log')  # Verwende logarithmische Skala für die Lernrate\n",
    "    \n",
    "    ax2.tick_params('y', colors='black')\n",
    "    ax2.yaxis.set_major_formatter(StrMethodFormatter('{x:1.0e}'))\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.0, 0.95))  \n",
    "    \n",
    "    if save_fig:\n",
    "        plt.title('Name: '+name)\n",
    "        path = os.path.join(dir_path, name + '_history.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d89800205cdc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dir(path_to_dir):\n",
    "    isExist = os.path.exists(path_to_dir)\n",
    "    if not isExist:\n",
    "        os.makedirs(path_to_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c285abdfcc81c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A utility function to decode the output of the network.\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search.\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:, :load_transfer_data.max_len]\n",
    "    # Iterate over the results and get back the text.\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n",
    "        res = tf.strings.reduce_join(tokenizer.num_to_char(res)).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2678f21bfbbb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_evaluation(name, dir_path, save_fig):\n",
    "    for batch in val_ds.take(1):\n",
    "        batch_images = batch[\"image\"]\n",
    "        _, ax = plt.subplots(4, 4, figsize=(32, 8))\n",
    "\n",
    "        preds = prediction_model.predict(batch_images)\n",
    "        pred_texts = decode_batch_predictions(preds)\n",
    "\n",
    "        for i in range(16):\n",
    "            img = batch_images[i]\n",
    "            img = tf.image.flip_left_right(img)\n",
    "            img = tf.transpose(img, perm=[1, 0, 2])\n",
    "            img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n",
    "            img = img[:, :, 0]\n",
    "\n",
    "            title = f\"Prediction: {pred_texts[i]}\"\n",
    "            ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n",
    "            ax[i // 4, i % 4].set_title(title)\n",
    "            ax[i // 4, i % 4].axis(\"off\")   \n",
    "    if save_fig:\n",
    "        path = os.path.join(dir_path, name + '_result.png')\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46158f62a4aac633",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d958dca138b01f54",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_new_plot_name(model_name, names, format):\n",
    "    import re\n",
    "    pattern = r\"\\d+\"\n",
    "    max_number = 0\n",
    "    for name in names:\n",
    "        tmp_name = name.replace(model_name,\"\")\n",
    "        number = int(re.findall(pattern,tmp_name)[0])\n",
    "        if number > max_number:\n",
    "            max_number = number\n",
    "            \n",
    "    new_model_name = model_name + \"V_\" + str(max_number + 1)\n",
    "    return format.replace(model_name,new_model_name)\n",
    "        \n",
    "if not os.path.exists(TEST_RESULT_DIR_NAME):\n",
    "            create_dir(TEST_RESULT_DIR_NAME)\n",
    "files_with_model_name = [file for file in os.listdir(TEST_RESULT_DIR_NAME) if MODEL_NAME in file]\n",
    "metrics = history.history\n",
    "\n",
    "NAME = \"{name}_{epoch}E_{height}H_{width}W_{loss}L_{val_loss}VL_{time}s\".format(\n",
    "    name=MODEL_NAME, epoch=history.epoch[-1], height=IMAGE_HEIGHT, width=IMAGE_WIDTH,\n",
    "    loss=round(metrics['loss'][-1],2), val_loss=round(metrics['val_loss'][-1], 2), time=round(total_duration))\n",
    "\n",
    "if not files_with_model_name:\n",
    "    if SAVE_HISTORY:\n",
    "        plot_history(history, NAME, TEST_RESULT_DIR_NAME, True)\n",
    "        plot_evaluation(NAME, TEST_RESULT_DIR_NAME, True)\n",
    "else:\n",
    "    new_name = create_new_plot_name(MODEL_NAME,files_with_model_name, NAME)\n",
    "    plot_history(history, new_name, TEST_RESULT_DIR_NAME, True)\n",
    "    plot_evaluation(new_name, TEST_RESULT_DIR_NAME, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca800c",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde914d352e2192",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if MODEL_SAVE:\n",
    "    if not os.path.exists(MODEL_DIR_NAME):\n",
    "        create_dir(MODEL_DIR_NAME)\n",
    "    model_path = os.path.join(MODEL_DIR_NAME, \"{model_name}\".format(model_name=MODEL_NAME))\n",
    "    model.save(model_path)\n",
    "    model.save_weights(os.path.join(model_path, f\"{MODEL_NAME}{weights_keras_string}\"), overwrite=True, save_format=None, options=None)\n",
    "    json_string = model.to_json()\n",
    "\n",
    "    with open(os.path.join(model_path, f\"{MODEL_NAME}.json\"),'w') as f:\n",
    "        f.write(json_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

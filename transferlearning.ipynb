{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:11.144451500Z",
     "start_time": "2024-01-13T10:48:08.766968400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:11.152909800Z",
     "start_time": "2024-01-13T10:48:11.145903Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:11.987445300Z",
     "start_time": "2024-01-13T10:48:11.981436300Z"
    }
   },
   "outputs": [],
   "source": [
    "characters = set()\n",
    "max_len = 0\n",
    "base_path = \"data_zettel/cropped_images/\"  # gets overwritten by config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:13.444580600Z",
     "start_time": "2024-01-13T10:48:13.441574100Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    data_list = []\n",
    "    image_files = [f for f in os.listdir(base_path) if f.endswith('.jpg')]\n",
    "\n",
    "    for image_file in image_files:\n",
    "        image_name = os.path.splitext(image_file)[0]\n",
    "\n",
    "        img_path = os.path.join(base_path, image_file)\n",
    "        label_file = os.path.join(base_path, f\"{image_name}.txt\")\n",
    "\n",
    "        if os.path.exists(label_file):\n",
    "            with open(label_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                line = file.readline().strip()\n",
    "                data_list.append((img_path, line))\n",
    "\n",
    "    #np.random.shuffle(data_list) # Rausgenommen zum testen\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:14.175368600Z",
     "start_time": "2024-01-13T10:48:14.171828800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocabulary_length(data):\n",
    "    characters = set()\n",
    "    max_len = 0\n",
    "\n",
    "    for _, label in data:\n",
    "        for char in label:\n",
    "            characters.add(char)\n",
    "\n",
    "        max_len = max(max_len, len(label))\n",
    "\n",
    "    characters = sorted(list(characters))\n",
    "\n",
    "    print(\"Maximum length: \", max_len)\n",
    "    print(\"Vocab size: \", len(characters))\n",
    "    return characters, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:17.140926800Z",
     "start_time": "2024-01-13T10:48:17.135917500Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_paths_and_labels(samples):\n",
    "    x_img_paths = []\n",
    "    y_labels = []\n",
    "\n",
    "    for img_path, label in samples:\n",
    "        if os.path.exists(img_path):\n",
    "            x_img_paths.append(img_path)\n",
    "            y_labels.append(label)\n",
    "\n",
    "    return x_img_paths, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:17.913035Z",
     "start_time": "2024-01-13T10:48:17.908522900Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(lines_list):\n",
    "    split_idx = int(0.9 * len(lines_list))\n",
    "    train_samples = lines_list[:split_idx]\n",
    "    test_samples = lines_list[split_idx:]\n",
    "\n",
    "    val_split_idx = int(0.5 * len(test_samples))\n",
    "    validation_samples = test_samples[:val_split_idx]\n",
    "    test_samples = test_samples[val_split_idx:]\n",
    "\n",
    "    return train_samples, test_samples, validation_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:20.980562800Z",
     "start_time": "2024-01-13T10:48:20.282062400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:  47\n",
      "Vocab size:  71\n"
     ]
    }
   ],
   "source": [
    "data = read_data()\n",
    "all_data = read_data()\n",
    "characters, max_len = get_vocabulary_length(all_data)\n",
    "train_samples, test_samples, validation_samples = split_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:39.938140400Z",
     "start_time": "2024-01-13T10:48:39.934627500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train samples: 1983\n",
      "Total validation samples: 110\n",
      "Total test samples: 111\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total train samples: {len(train_samples)}\")\n",
    "print(f\"Total validation samples: {len(validation_samples)}\")\n",
    "print(f\"Total test samples: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:42.614260300Z",
     "start_time": "2024-01-13T10:48:42.537261900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train_img_paths, y_train_labels = get_image_paths_and_labels(train_samples)\n",
    "\n",
    "x_val_img_paths, y_val_labels = get_image_paths_and_labels(validation_samples)\n",
    "\n",
    "test_path, test_label = get_image_paths_and_labels(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-13T10:48:49.605584500Z",
     "start_time": "2024-01-13T10:48:48.675075400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:  47\n",
      "Vocab size:  71\n"
     ]
    }
   ],
   "source": [
    "# Has to be here because load data functions need to be called before\n",
    "import handwriting.tokenizer as tokenizer\n",
    "import handwriting.custom_image_generator as cgi\n",
    "\n",
    "# takes eternity\n",
    "#x_train, y_train = tokenizer.prepare_data(x_train_img_paths, y_train_labels) \n",
    "#x_test, y_test = tokenizer.prepare_data(x_test_img_paths, y_test_labels)\n",
    "\n",
    "#train_generator = cgi.CustomImageGenerator(x_train_img_paths, y_train_labels, BATCH_SIZE, IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "train_ds = tokenizer.prepare_dataset(x_train_img_paths, y_train_labels, (512,64), 32)\n",
    "val_ds = tokenizer.prepare_dataset(x_val_img_paths, y_val_labels,(512,64), 32)\n",
    "#test_ds = tokenizer.prepare_dataset(x_test_img_paths, y_test_labels,(IMAGE_WIDTH,IMAGE_HEIGHT),BATCH_SIZE)\n",
    "#aug_train_ds = tokenizer.prepare_augmented_dataset(x_train_img_paths, y_train_labels, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ä', 'Ö', 'ß', 'ä', 'ö', 'ü']\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-13T10:51:46.869755400Z",
     "start_time": "2024-01-13T10:51:46.861244Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-htr-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
